---
title: "Investigating lexical-semantic effects on morphosyntactic variation using elastic net regression"
author:
  - name: Anthe Sevenants
    email: anthe.sevenants@kuleuven.be
    orcid: 0000-0002-5055-770X
    affiliations:
      - name: KU Leuven
  - name: Freek Van de Velde
    email: freek.vandevelde@kuleuven.be
    orcid: 0000-0003-3050-2207
    affiliations:
      - name: KU Leuven
  - name: Dirk Speelman
    email: dirk.speelman@kuleuven.be
    orcid: 0000-0003-1561-1851
    affiliations:
      - name: KU Leuven
format:
  html:
    toc: true
  docx:
    toc: false
editor: source
title-block-banner: true
bibliography: references.bib
toc: true
toc-depth: 4
toc-location: left
tbl-cap-location: bottom
fig-cap-location: bottom
number-sections: true
reference-location: margin
csl: chicago-author-date.csl
df-print: kable
execute:
  echo: false
---

```{css, echo = FALSE}
.center {
  text-align: center !important
}
```

```{r initialisation}
#| output: false
library(dplyr)
library(scales)
library(magrittr)
library(broom)
library(tidycat)
library(emmeans)
library(forcats)
library(glossr)
use_glossr()

formatn <- function(number) {
    format(round(as.numeric(number), 1), nsmall=0, big.mark=",")
}

formatd <- function(number, nsmall=2) {
    format(round(number, nsmall), nsmall=nsmall)
}

formatp <- function(number) {
    label_percent()(number)
}

suppress_gloss <- TRUE
```

## Introduction

### Studying alternate ways of saying the same thing

Whenever there are "alternate ways of saying the same thing" [@labov_sociolinguistic_1972, 188] in a language, linguists take great interest. As such, several approaches have been developed to try and explain the choice between these "alternate ways" as accurately as possible. Generally speaking, we can distinguish between two broad theoretical lines to approach research on morphosyntactic variation. The first theoretical line is characterised by a focus on high-level, general predictors. In such analyses, we find predictors with a limited number of levels, sometimes pooled from more narrow categories. Methodologically, these analyses are carried out using (mixed-effects) regression analyses [e.g. @gries_most_2015]. The second theoretical line is characterised by a focus on frequency, and frequency only. In such analyses, the pull of lexical items towards a specific morphosyntactic variant is computed. Methodologically, these analyses are carried out as collostructional analyses [@gries_collostructional_2013], or derivatives of this technique.

The issue with these two lines of research is that they inadvertently have a bias for one specific type of predictor. The type of analysis typically carried out with regression analysis promotes the use of high-level, clear-cut predictors, while the lexical frequency research done with collostructional analysis is solely focussed on lexical items. This puts these probabilistic models into a pickle, as they are forced into highlighting only part of the phenomena under scrutiny. Of course, the theoretical choice between these two approaches has its repercussions, as predictors of different natures are never present as equals in the same models, which means linguistic conclusions are usually only centred around one or the other -- never both at once. Of course, these results are still valid in the strict sense, but they never paint a full picture. In this article, we will showcase elastic net regression, a regression technique which can handle both high-level and lexical predictors at once in a single model. We will demonstrate this technique through a case study of a morphosyntactic alternance in Dutch. First, however, we will discuss previous attempts to bring the lexical dimension into models of variation.

### Previous lexical effects research

There are several techniques available which attempt to model lexical influence in some way, but these techniques have not been without major drawbacks. One option explored by @bloem_processing_2021 is to investigate the unexplained variation or so-called *residuals* of each data point in a logistic regression analysis. The idea is that the mean residuals for a specific type (e.g. the residuals for all data points with the same word) hint at the lexical association of that type towards an option of a specific alternance [@bloem_processing_2021 118, 135]. While this approach to measuring lexical-semantic effects has been shown to work, it lumps together unexplained variation and lexical-semantic effects into one category, which is statistically infelicitous. Philosophically, it also still regards lexical items as "lesser" predictors, since they are not included in the main analysis and are only considered in a post-hoc manner.

A current state-of-the-art analysis would attempt to use random effects in a mixed-effects logistic regression [cf. @gries_most_2015]. In such an approach, specific lexical verbs partaking in an alternance are seen as levels of a factor for which, usually, one extra parameter is fitted, as a random factor. This would result in a correction term for each type separately, which would indicate how specific words contribute towards the choice between two constructions. However, as @van_de_velde_investigating_2019 [168-169] describe, there are generally two issues with this approach. The first issue is that using random effects in this way is, strictly speaking, conceptually unsound. Random effects are meant to account for structural associations in the data, to avoid Type I errors. If one were to use the by-word corrections from the random factor as a variable of interest, to boost the statistical power of the analysis (avoiding Type II errors), this would amount to making one's model less conservative, letting Type I errors sneak in via the backdoor. If the lexical verbs are a variable of interest, one should make them part of the *fixed effects structure* and have them function on the same level as the high-level predictors.

This brings us to the second issue raised by @van_de_velde_investigating_2019; there are *many* possible lexical types, and word distribution is notoriously skewed [@zipf_psycho-biology_1965]. As @bloem_processing_2021 [117] says, this results in "an impossible task when there are many unique \[words\] -- you can't create a line from a single data point" -- and models will fail to converge given too many lexical types. A possible solution to this problem of high dimensionality would be to use some sort of "semantic binning", i.e. by grouping individual words into larger semantic categories. However, this means we lose the ability to look at lexical effects at the level of individual words, and this also introduces bias into the analysis at the level of the grouping itself, unless you use clusters, like in @levshina_radically_2014 or @van_de_velde_investigating_2019.

Currently, then, there is no technique which handles both high-level predictors and lexical items as full-fledged predictors of equal value. In this article, we demonstrate elastic net regression, a new methodology which is able to process predictors of various nature on an equal footing. To demonstrate this technique, we will build an all-encompassing, holistic model of the so-called "red" and "green" word order in Dutch, with both the high-level predictors found in previous research as well as lexical items. In an attempt to find patterns in the semantic preferences of the verbs occurring in the verbal clusters, we will make use of semantic vectors as well as a lexical-semantic database. First, however, we will explain what the elastic net technique does and why it is so useful for fairer research on morphosyntactic variation.

## Elastic net regression {#sec-elastic-net-regression}

Elastic net regression is a regression analysis technique which incorporates so-called "regularisation" into its model fitting procedure. In contrast to "more traditional" regression techniques like linear or logistic regression, elastic net regression adds a penalisation factor to the maximum likelihood procedure used to compute the model parameters. In practice, this means that the model coefficients will be "shrunk" in order to guarantee fairer and more generalisable predictions, avoiding overfitting. By additionally using cross-validation (see below), we can fit many more variables than using traditional regression, even to the extent that we can have a dataset with $n$ observations and $p$ variables, where $n < p$. For an analysis with potentially hundreds of words as predictors, this becomes a powerful technique. High-level, traditional predictors remain part of the analysis, which means the analysis can remain under multifactorial control.

In addition, model coefficients can be shrunk to the extent that they are reduced to zero, effectively disabling the predictors of those coefficients outright. This behaviour is very welcome in high-dimensional situations, like when entering hundreds of words as predictors for an analysis including lexical-semantic predictors. This allows us to effectively carry out variable selection *within* the regression analysis itself.

### Components of elastic net regression

When we speak of elastic net regression, we actually mean the combination of "lasso regression" and "ridge regression". Both are penalised regression techniques, but their exact operationalisations are different.

Lasso regression attenuates predictions through the use of the so-called L1-norm.[^lnote] In lasso regression's objective function, the coefficients of the model are optimised for maximum likelihood, like in a traditional regression fitting procedure. Additionally, however, the absolute values of the coefficients are also used as a *penalty* to the same objective function. An additional parameter lambda ($\lambda$) controls how severe this penalty is. The following equation shows the lasso objective function for a binary response variable [adapted from @friedman_regularization_2010]:
$$
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} \underbrace{ - \biggl[ \frac{1}{N} \cdot \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \cdot \beta) - \log (1+e^{(\beta_0+x_i^T \cdot \beta)})\biggr] }_{\text{negative log-likelihood of logistic regression}} + 
 \underbrace{\biggl[ \lambda \cdot \lvert \beta \rvert \biggr]}_{ \text{regularization penalty }}
$$

[^lnote]: The L1- and L2-norm (further down) are mathematical concepts that are not to be confused with the terms L1 and L2 referring to language acquisition. We will not use L1 and L2 in this latter context in this article.

::: {layout-ncol="2"}

:::: {#first-column}
**Negative log-likelihood**

- $\beta_0$: intercept
- $\beta$: vector of regression coefficients
- $p$: number of predictor variables
- $N$: number of observations
- $\frac{1}{N}$: normalise loss
- $\sum_{i=1}^N$: for each observation
- $y_i$: binary response variable for the ð‘–^th^ observation
- $x_i$: vector of predictor variables for the ð‘–^th^ observation
:::

::: {#second-column}
**Regularization penalty**

- $\lambda$: controls the strength of the penalty
- $\lvert \beta \rvert$: L1 regularisation
	- adds the sum of the absolute values of the coefficients as a penalty term
:::

::::

Ridge regression, lasso's sibling, attenuates predictions through the L2-norm. This procedure is comparable to the L1 used in lasso regression. For ridge regression, the coefficients are *squared* to serve as a penalty. Here as well, the severity of the penalty is controlled using the additional parameter lambda. We show the ridge objective function for a binary response variable [adapted from @friedman_regularization_2010]:
$$
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} \underbrace{ - \biggl[ \frac{1}{N} \cdot \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \cdot \beta) - \log (1+e^{(\beta_0+x_i^T \cdot \beta)})\biggr] }_{\text{negative log-likelihood of logistic regression}} + 
 \underbrace{\biggl[ \lambda \cdot \beta^2 \biggr]}_{ \text{regularization penalty }}
$$

::: {layout-ncol="2"}

:::: {#first-column}
**Negative log-likelihood**

- $\beta_0$: intercept
- $\beta$: vector of regression coefficients
- $p$: number of predictor variables
- $N$: number of observations
- $\frac{1}{N}$: normalise loss
- $\sum_{i=1}^N$: for each observation
- $y_i$: binary response variable for the ð‘–^th^ observation
- $x_i$: vector of predictor variables for the ð‘–^th^ observation
:::

::: {#second-column}
**Regularization penalty**

- $\lambda$: controls the strength of the penalty
- $\beta^2$: L2 regularisation
	- adds the sum of the squared values of the coefficients as a penalty term
:::

::::

While both lasso and ridge regression shrink coefficients, only lasso regression can actually bring a coefficient to zero. In ridge regression, coefficients can, mathematically, only be brought *asymptotically close* to zero. Which technique is the best depends on the exact use case.

### Deciding on lambda and the split between lasso and ridge regression

Both lasso and ridge regression use the lambda parameter to determine the severity of the penalty applied during the model fitting process. This lambda value can range from $0$ (no penalty) to $+\infty$ (maximal penalty). The question then arises how one decides what value to use for lambda in order to obtain the best possible model. To compute lambda, we typically rely on *k-fold cross-validation*, a common practice in the machine learning field. We divide our dataset into *k* "folds", effectively "slices" of data. For example, in 10-fold cross validation, the dataset will be divided into ten parts. First, we use the nine folds to fit the model ("training set") and try a wide array of different lambda values. To examine how well each lambda value performs, we predict values using the fitted model and test how much the predictions deviate from the real values in the remaining fold ("testing set"). We then repeat this process nine more times, each time with a different fold used as a testing set. This enables us to get a global estimation of how well each possible lambda value performs, which helps combat overfitting. The lambda value which produces the best predictions across all fold combinations is chosen to fit a model on the *entire* dataset.

We saw earlier that lasso and ridge regression both have their own strengths. Elastic net regression leverages those strengths and *combines* both techniques. This means that both L1 and L2 regularisation are used at the same time, with an additional parameter alpha ($\alpha$) regulating the split between lasso and ridge penalty [adapted from @friedman_regularization_2010]:
$$
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} \underbrace{ - \biggl[ \frac{1}{N} \cdot \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \cdot \beta) - \log (1+e^{(\beta_0+x_i^T \cdot \beta)}) \biggr] }_{\text{negative log-likelihood of logistic regression}} +
 \underbrace{\lambda \cdot  \biggl[ \alpha \cdot \|\beta\|_1 + (1-\alpha) \cdot \|\beta\|_2^2/2  \biggr] }_{\text{regularization penalty}}
$$

::: {layout-ncol="2"}

:::: {#first-column}
**Negative log-likelihood**

- $\beta_0$: intercept
- $\beta$: vector of regression coefficients
- $p$: number of predictor variables
- $N$: number of observations
- $\frac{1}{N}$: normalise loss
- $\sum_{i=1}^N$: for each observation
- $y_i$: binary response variable for the ð‘–^th^ observation
- $x_i$: vector of predictor variables for the ð‘–^th^ observation
:::

::: {#second-column}
**Regularization penalty**

- $\lambda$: controls the strength of the penalty
- $\alpha$: determines the balance between L1 and L2 regularisation
- $\|\beta\|_1$: L1 norm
	- = $\|x\|_1$ with $x \in \mathbb{R}^n$ = $\|x_1\| + ... + \|x_n\|$
- $\|\beta\|_2^2/2$: adjusted L2 norm
	- = $\|x\|_2$ with $x \in \mathbb{R}^n$ = $\sqrt{x_1^2 + ... + x_n^2}$
	- division by two balances the regularization penalty
:::

::::

Alpha values range between $0$ (only ridge) and $1$ (only lasso). Values between $0$ and $1$ dictate ridge-lasso splits. In this way, ridge and lasso can be deployed optimally for the problem at hand. There is no optimality algorithm for selecting the best alpha, so the best alpha value is decided by testing ranges of different alphas, for example by making models from $\alpha = 0$ to $\alpha = 1$ in increments of $0.2$ (resulting in eight models total). For each of those models, a loss is computed, a measure of the difference between a model's predictions and the actual data. The model with the alpha which produces the lowest loss, so the smallest difference between the real data and the predictions, is chosen.

## "Red" and "green" word order in Dutch subordinate sentences

As mentioned before, we will showcase the elastic net regression technique through a case study on morphosyntactic variation in Dutch. Traditionally in linguistics, there are long-standing theories that "different forms must have [also] different meanings" [@haiman_iconicity_1980, 515-516], be it denotationally, pragmatically or socially [@croft_construction_2010, 472]. In this regard, the verbal clusters found in many Germanic languages, constructions which feature multiple verbal items [@wurmbrand_verb_2017, 4611], are especially interesting: they often feature variation which is essentially free, without any true meaning differences. More specifically in Dutch, we find the alternance between the "red" and "green" word order, a very well-researched morphosyntactic alternance in Dutch. The gist is that in verbal clusters, two verb orders are possible:

```{r, red-gloss}
#| tbl-cap: Red word order example
if (!suppress_gloss) {
red_example <- as_gloss(
  "Vandaar dat hij ons opnieuw is~AUX~ aangeboden~PTCP~.",
  "hence that he us again was offered",
  translation = "Hence why he was offered to us again.",
  label = "red-example",
  source = "[AUX + PTCP]: red order (WR-P-P-G-0000250935.p.3.s.1[^gloss-source])"
)
red_example
}
```

```{r, green-gloss}
if (!suppress_gloss) {
#| tbl-cap: Green word order example
green_example <- as_gloss(
  "Zolang ge god maar aanbeden~PTCP~ hebt~AUX~.",
  "as-long-as you God but worshipped have",
  translation = "As long as you've worshipped God",
  label = "green-example",
  source = "[PTCP + AUX]: green order (WR-P-E-A-0000223098.p.4.s.2)"
)
green_example
}
```

[^gloss-source]: All examples reference the SoNaR corpus [@oostdijk_d-coi_2008] part they come from.

This alternance is also possible in main clauses with three or more verbs, but it is best known in subordinate clauses. Crucially, a difference in word order does not appear to entail a difference in meaning [@bloem_processing_2021, 2].

### Formal accounts

Verb cluster variation such as the red and green word order are of high interest to formal syntactic theorists, since they pose fundamental challenges to the formal syntactic apparatus. More specifically, in formal linguistic theory, a choice can be made to opt either for a head-initial or a head-final syntactic design [@augustinus_complement_2015, 46]. In a head-initial design, the head of a phrase is always generated first in that phrase, with its complements coming second. In contrast, in a head-final design, the complements of a phrase come first, and the head of those complements comes after. The choice between head-initial or head-final has repercussions for the so-called "base order" in verb clusters. A head-initial design generates the red word order in Dutch by default: the auxiliary, which serves as the head in the verbal phrase, is generated first, and selects the participle as its complement. A head-final design generates the green word order in Dutch by default: the auxiliary is generated last, and the participle it selects as its complement comes before it. West-Germanic languages are commonly thought of as head-final [@wurmbrand_verb_2017, 4614], but head-initial accounts also exist [@zwart1993dutch].

Of course, a formal grammar should be able to cover both word orders which naturally occur in Dutch, regardless of head location. @wurmbrand_verb_2017 shows two strategies to achieve this coverage. The first strategy is to ignore verb cluster word order variation as being part of the syntax. In this strategy, the actual verb cluster order is determined only *after* the sentence deep structure is generated [@wurmbrand2004syntactic]. This is done through a mechanism called "Flip". This mechanism swaps sister nodes, such as an auxiliary and a participle under the same node. Through this technique, any of our two word orders can be derived, regardless of the base order. This post-syntactic strategy is motivated on semantic terms: a change in order has no fundamental semantic consequences, and therefore the distinction is not considered important to have represented in the actual, deep syntax tree.

The second strategy is to use movement, a common device in formal syntactic theory where elements in the deep structure move to other locations, sometimes iteratively, as the result of some language-internal trigger. There is no shortage of possible movements, regardless of head location. For example, in a head-final architecture (corresponding to the green word order), the red word order can be derived through *verb raising* [@evers_transformational_1975], a process in which the participle moves upwards to the auxiliary and attaches to the right.

Theoretical-formal accounts, however, have not yet been able to find an explanation for the occurrence of either the red or green word order. While there are ample theoretical devices to derive any word order from any base order, even for highly complex verbal clusters, there are no theoretical grounds for explaining the possible variation between red and green. Crucially, if the choice between two variants is free, there cannot be a trigger for movement, yet movement is compulsory in order to be able to derive the non-base word order.

### Probabilistic accounts {#sec-probabilistic-accounts}

Usage-based studies of the red-green phenomenon are not subject to the aforementioned theoretical movement issues, and instead attempt to explain verb cluster variation using probabilistic models. In this section, we will give an overview of the different probabilistic influences that have been found so far for the red and green word order.

#### Influence of lectal factors

Context is an important influence in the choice between the red and green word order, with a general distinction between two types of contexts. The first is the *regional* context: @de_sutter_rood_2005 [68] found that the red word order is much less preferred in Belgium than in the Netherlands, a result corroborated by @bloem_processing_2021 [206]. The second context is register: in spoken, interactional and unedited language use, the green order is much more prevalent [@de_sutter_rood_2005, 81-87]. Inversely, this means that formal, unidirectional and edited language favours the red word order.

#### Influence of prosody

Another source of influence is that of prosody. The most important prosodical influence [@de_sutter_rood_2005, 305] stems from the so-called "separable verbs" in Dutch. These verbs, comparable to phrasal verbs in English, have a preposition which can be detached from the root.[^separable-example] These word-initial prepositions always carry stress, giving rise to a chain of two adjacent stresses. @de_sutter_rood_2005 [154] found that speakers often choose the red order over the green order in order to avoid this chain. Consider the following example:

```{r, prosody-green-gloss}
#| tbl-cap: Green word order prosody example
if (!suppress_gloss) {
prosody_green_example <- as_gloss(
  "dat ik hier mijn selectie Â°definitief Â°afgedwongen heb .",
  "that I here my selection definitively enforced have .",
  translation = "that I have definitively enforced my selection here.",
  label = "prosody_green-example",
  source = "[PTCP + AUX] adapted green order (WR-P-P-G-0000198602.p.7.s.4)"
)
prosody_green_example
}
```

If both *definitief* 'definitively' and *afgedwongen* 'enforced' are stressed, as they would typically be in Dutch, there would be two consecutive stresses. If one uses the red word order, this chain of stresses can be avoided:

```{r, prosody-red-gloss}
#| tbl-cap: Red word order prosody example
if (!suppress_gloss) {
prosody_red_example <- as_gloss(
  "dat ik hier mijn selectie Â°definitief heb Â°afgedwongen .",
  "that I here my selection definitively have enforced .",
  translation = "that I have definitively enforced my selection here.",
  label = "prosody_red-example",
  source = "[AUX + PTCP] actual red order (WR-P-P-G-0000198602.p.7.s.4)"
)
prosody_red_example
}
```

[^separable-example]: e.g. *ik zoek~root~ het adres op~preposition~*, literally 'I search up the address'

Another prosody-related effect is the length of the "middle field" in Dutch. In our case, in subordinate sentences, the middle field is the area between the conjunction and the verbal cluster ("ik hier mijn selectie definitief" in the example above). Both in @de_sutter_rood_2005 [153-154] and @bloem_processing_2021 [25] it is found that the longer this middle field is, the higher the probability is of choosing the red word order. @de_sutter_rood_2005 [153] hypothesise that a longer middle field gives rise to a higher probability of consecutive stresses, which, as stated above, can be remedied by using the red word order.

#### Influence of frequency

@de_sutter_rood_2005 [289-295] found that the frequency of a participle's lemma positively correlates with the probability of that participle appearing in the red order. @bloem_processing_2021 [24] repeated the experiment and also found an effect in the same direction. @de_sutter_rood_2005 [323-324] argue that frequently appearing participles are easier to recover from memory, leaving more time for the language user to spend on (more difficult) stylistic encoding, which @de_sutter_rood_2005 then assume is the red order. 

#### Influence of priming

Another influence on the choice between red and green is priming. @de_sutter_rood_2005 [278] found that a word order is much more likely to be used if it is preceded by another verb cluster in the same order. Priming is considered to be the second most important language-internal influence in the choice between the two word orders [@de_sutter_rood_2005, 305].

#### Influence of semantics {#sec-semantics-influence}

Despite extensive research on red and green, semantics is an underdeveloped researched area for this alternance [@bloem_processing_2021]. The most extensive semantic research concerns macroscopic research of some semantic feature, in this case *adjectiveness*. It has been found that the more a participle is used as an adjective, the more likely this participle is to be used in the green order [@de_sutter_rood_2005, 233-242]. This is logical, since adjectives can only appear in the green order in subordinate clauses. The choice between red and green is thus "contaminated" by the syntactic rules of the adjective [@pijpops_constructional_2018, 286-290]. Adjectiveness is thought to be the most important language-internal factor in deciding between the red and green word order [@de_sutter_rood_2005, 305], which shows that semantics is indeed an important factor to include in a morphosyntactic analysis.

Another semantic factor is the difference between "dynamic" and "stative" verbs. In the *Algemene Nederlandse Spraakkunst* [@haeseryn_30321_1997, 'General Dutch Grammar'], a descriptive reference grammar of Dutch, dynamic verbs are characterised as encoding an activity or process, while stative verbs are characterised as encoding states. @pardoen1991interpretatie launched the idea that the green order yields a stative interpretation of a verb cluster, while the red order yields a dynamic interpretation. @de_sutter_rood_2005 state the precarious nature of the evidence for Pardoen's hypothesis, as it is largely based on introspection [-@de_sutter_rood_2005, 203]. In addition, through a disctinctive collexeme analysis, they find that certain words like *geweest* ('been') or *gehad* ('had') associate strongly with the red word order, yet they find it difficult to see how one would frame a dynamic interpretation of these intrinsically stative verbs. As such, they argue that the aforementioned association posited by Pardoen may not be very strict, or that it should at least be interpreted separately from the semantics of the main verb. @de_sutter_rood_2005 thus do not reject the hypothesis by @pardoen1991interpretatie entirely, and instead opt to call for modifications to the original, strict idea.

Both @de_sutter_rood_2005 and @bloem_processing_2021 also explored the lexical influence on the choice between red and green. In these studies, associations between certain verbs and a preference for either word order were found. In a further semantic analysis, @bloem_processing_2021 [131] looked at the semantic properties from the Cornetto semantic database for Dutch [@piek2013cornetto] and found no striking difference in the frequency distribution between red and green of verbs tagged as "dynamic". Instead, he founds that verbs tagged as "cognitive" seem to appear in the green word order more often, as well as verbs rated as "negative" by an automatic sentiment analysis tool. However, these results must be interpreted with caution, as they are only based on 40 verbs and are not statistically underpinned either.

#### Influence of auxiliary verb

Finally, the auxiliary verb used in tandem with the verb cluster also has an influence on the choice between the red and green order. @de_sutter_rood_2005 [257] found that the green word order is more likely when a verbal cluster occurs with auxiliary verb *zijn* 'to be', than when it occurs with *hebben* 'to have' or *worden* 'to become'. Because *zijn* is a copular verb in Dutch, it is strongly associated with adjectival contexts which, as already stated, can only appear in the green word order in subordinate clauses (e.g. *omdat een marsman groen is*, 'because a martian is green'). An analogy process thus carries over this preference to verb cluster order. The other auxiliaries (*hebben* and *worden*), comparatively, are more associated with the red word order. We see the same tendencies in @bloem_processing_2021 [25].

## Data

For our model, we collected all red and green verb clusters in subordinate clauses in the SoNaR Corpus [@oostdijk_d-coi_2008] and SoNaR New Media Corpus [@oostdijk_sonar_2014]. For more information about the data collection process see the Appendix.

```{r filtered-attestations-df-load}
df_filtered <- read.csv("output/RoodGroenAnthe_sampled.csv")
```

```{r types-count}
types_count <- df_filtered$participle %>% unique %>% length
```

After filtering invalid occurrences, `r nrow(df_filtered) %>% formatn` attestations of the red and green word order remained, belonging to `r types_count %>% formatn` unique types.[^types] `r subset(df_filtered, order == "red") %>% nrow %>% formatn` attestations belong to the red order, `r subset(df_filtered, order == "green") %>% nrow %>% formatn` to the green order. `r subset(df_filtered, country == "BE") %>% nrow %>% formatn` attestations come from Belgium, `r subset(df_filtered, country == "NL") %>% nrow %>% formatn` attestations from the Netherlands. The imbalance between Belgium and the Netherlands stems from an overall imbalance between Belgian and Netherlandic material in SoNaR. This highlights why it is so important to have a multifactorial technique, like elastic net regression, which can account for these kinds of issues.

[^types]: Types, in this case, refer to verbal participles we find in the verb clusters.

```{r auxiliary-distribution}
auxiliary_distribution <- xtabs(~ auxiliary_lemma, data=df_filtered)
auxiliary_distribution_prop <- auxiliary_distribution %>% prop.table()
```

```{r separability-distribution}
separability_distribution <- xtabs(~ separable, data=df_filtered)
separability_distribution_prop <- separability_distribution %>% prop.table()
```

```{r editing-distribution}
editing_distribution <- xtabs(~ edited, data=df_filtered)
editing_distribution_prop <- editing_distribution %>% prop.table()
```

For an overview of the proportions and distributions of the high-level predictor values, see @fig-descriptive-plots. The operationalisation of our high-level predictors is available in the Appendix. The mean adjectiveness among all attestations is `r df_filtered$adjectiveness %>% mean() %>% formatd()` (median `r df_filtered$adjectiveness %>% median() %>% formatd()`). 
Of all attestations, `r auxiliary_distribution[["hebben"]] %>% formatn` verb clusters (`r auxiliary_distribution_prop[["hebben"]] %>% formatp`) feature the auxiliary *hebben* 'to have' in the verb cluster, 
`r auxiliary_distribution[["zijn"]] %>% formatn` (`r auxiliary_distribution_prop[["zijn"]] %>% formatp`) the auxiliary *zijn* 'to be' and 
`r auxiliary_distribution[["worden"]] %>% formatn` (`r auxiliary_distribution_prop[["worden"]] %>% formatp`) the auxiliary *worden* 'to become'. 
`r separability_distribution[["TRUE"]] %>% formatn` verb clusters (`r separability_distribution_prop[["TRUE"]] %>% formatp`) contain a separable verb. 
The mean priming rate among our attestations is `r df_filtered$priming_rate %>% mean() %>% formatd()` (median `r df_filtered$priming_rate %>% median() %>% formatd()`). 
`r editing_distribution[["TRUE"]] %>% formatn` attestations (`r editing_distribution_prop[["TRUE"]] %>% formatp`) belong to a subcorpus with assumed edited language. 
Of course, because the SoNaR corpus is a corpus of written language, there is a skew towards edited language. 
The mean type frequency of the participles in the verb clusters is `r df_filtered$FREQcount %>% mean() %>% round() %>% formatn()` (median `r df_filtered$FREQcount %>% median() %>% round() %>% formatn()`). The mean length of the middle field is `r df_filtered$middle_field_length %>% mean() %>% round() %>% formatn()` (median `r df_filtered$log_middle_field_length %>% median() %>% round() %>% formatn()`).

```{r}
#| label: fig-descriptive-plots
#| fig-cap: "Overview of the proportions and distributions of the high-level predictor values" 
#| column: screen-inset-shaded
#| layout-nrow: 2
#| layout-ncol: 3
#| fig-subcap:
#|   - "Distribution of adjectiveness in the dataset"
#|   - "Proportion of auxiliary lemmas in the dataset" 
#|   - "Proportion of separable verbs in the dataset" 
#|   - "Distribution of priming rate in the dataset." 
#|   - "Proportion of edited and unedited genres in the dataset" 
#|   - "Frequency distribution of types in the dataset (log10 scale)" 
#|   - "Lengths of the middle field in the dataset (log10 scale)" 
options(scipen=999)

# Adjectiveness
boxplot(df_filtered$adjectiveness, horizontal=T)

# Auxiliary verbs
barplot(auxiliary_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, auxiliary_distribution))))

# Separable verbs
row.names(separability_distribution) <- c("non-separable", "separable")
barplot(separability_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, separability_distribution))))

# Priming rate
priming_colour <- function(value) {
  if (value == 0) {
    return("black")
  }
  
  ifelse(value > 0, "red", "green")
}
priming_colour <- Vectorize(priming_colour)

boxplot(df_filtered$priming_rate, horizontal = T)
# points(df_filtered$priming_rate, rep(0.5, length(df_filtered$priming_rate)), pch="|",
#        col=priming_colour(df_filtered$priming_rate))

# Edited-unedited
row.names(editing_distribution) <- c("unedited", "edited")
barplot(editing_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, editing_distribution))))

# Log frequency
boxplot(df_filtered$logfreq, horizontal=T)

# Log middle field
boxplot(df_filtered$log_middle_field_length, horizontal=T)
```

## Computing semantic pull using elastic net regression {#sec-computing-semantic-pull}

### Running elastic net regression

```{r meta-info-load}
source("ElasticToolsR/MetaFile.R")

model_meta <- meta.file(read.csv("output/model_meta.csv"))

alpha <- subset(model_meta$as.data.frame(), predicate == "alpha")[["object"]]
lambda <- subset(model_meta$as.data.frame(), predicate == "lambda")[["object"]]
```

The elastic net regression itself was carried using ElasticToolsR [@sevenants_elastictoolsr_2023], an R library written for this study which acts as a wrapper around the excellent *glmnet* package for R [@friedman_regularization_2010]. We built nine predictors into our model (see @tbl-predictors). Our high-level predictors are based on the influences found in @sec-semantics-influence. `participle` is distributed among `r types_count %>% formatn` different coefficients: one for each type. `auxiliary_type` works in the same way: there are separate coefficients for *hebben*, *zijn* and *worden*. The other predictors are "traditional" in the sense that they have one coefficient each.

In both @de_sutter_rood_2005 and @bloem_processing_2021 we find several additional factors which were not previously mentioned, but which also contribute to the choice between red and green: syntactic depth, information value, definiteness, extraposition and inherence (collocations and multi-word units). These influences were not included for two reasons. Firstly, the measured effects for these influences are small compared to the influences we discussed above. This brings us to our second, more pragmatic reason: scope. The focus of this study is to build a rich, fair model with special attention for lexical influences as full predictors. Our high-level predictors of course also remain important, but they mainly serve as multifactorial control. A further operationalisation of a plethora of smaller effects would detract from the main goal of the study, and would foray into the area of diminishing returns as well. In fact, we have treated all language-internal factors deemed by @de_sutter_rood_2005 [305] to be the most important in deciding between red or green, which should provide ample multifactorial control.

| predictor | type | possible values |
| --- | --- | --- |
| participle | distributed binary values for each type | in verb cluster/not in verb cluster |
| auxiliary_type | distributed binary values for each auxiliary | *zijn*, *hebben*, *worden* |
| country | binary | BE/NL | 
| separable | binary | yes/no |
| edited | binary | yes/no |
| adjectiveness | real number | $0$ to $1$ |
| priming_rate | real number | $-\infty$ to $+\infty$ |
| logfreq | natural number | $0$ to $+\infty$ |
| log_middle_length_field | natural number | $0$ to $+\infty$ |


: An overview of the predictors for the elastic net regression {#tbl-predictors}

```{r scientific-notation}
fancy_scientific <- function(l) {
    # turn in to character string in scientific notation
    l <- scientific(l, digits=4)
    # quote the part before the exponent to keep all the digits
    exponent <- gsub(".*e(.*)$", "\\1", l) %>% as.numeric()
    prefix <- gsub("(.*)e.*$", "\\1", l)
    # turn the 'e+' into plotmath format
    l <- paste0(prefix, "\\times 10^{", exponent, "}")
    
    return(l)
}
```

ElasticToolsR automatically finds the appropriate alpha and lambda values which deliver the best possible model. It does this by trying out many different alpha values, and then selecting the alpha value which produces the model with the lowest loss. Lambda is decided using 10-fold cross validation for each attempted alpha value. Our model for the red and green order performs the best at alpha = $`r alpha %>% formatd(0)`$ (so-called "lasso regresion") and lambda = $`r lambda %>% fancy_scientific()`$.

<!--

-->

### Results

```{r coefficients-data-frame}
coefficients <- read.csv("output/RoodGroenAnthe_coefficients.csv")
coefficients_count <- dim(coefficients)[[1]]

coefficients$removed <- ifelse(coefficients$coefficient == 0, TRUE, FALSE)

removed_distribution <- xtabs(~ removed, data=coefficients)
removed_distribution_prop <- removed_distribution %>% prop.table()

coefficients_non_zero <- coefficients[coefficients$removed == FALSE,]
coefficients_non_zero$order <- ifelse(coefficients_non_zero$coefficient > 0, "red", "green")
non_zero_distribution <- xtabs(~ order, data=coefficients_non_zero)
non_zero_distribution_prop <- non_zero_distribution %>% prop.table()
```

```{r chi-square-test-load}
source("3-1 Chi-square.R")
```

Out of the `r coefficients_count %>% formatn` coefficients in our elastic net model, 
`r removed_distribution[["TRUE"]] %>% formatn` (`r removed_distribution_prop[["TRUE"]] %>% formatp`) 
coefficients were removed and 
`r removed_distribution[["FALSE"]] %>% formatn` (`r removed_distribution_prop[["FALSE"]] %>% formatp`)
coefficients were retained (@fig-coefficient-plots-1).
All except for one removed coefficients belong to the `Participle` predictor, which means all but one of our "traditional" predictors were retained by the model.

#### Lexical effects {#sec-results-lexical}

We will first, however, look at our `Participle` predictor. The coefficients corresponding to each of the types in the verb cluster attestations give us an idea of whether those participles tend to the red or green order. Since our model has the red word order as response variable $1$, positive coefficients show logit corrections towards the red word order, while negative coefficients show logit corrections towards the green word order, under multivariate control of known higher-order effects listed in @tbl-other-coefficients. We can interpret these attractions as lexical-semantic pulls.
Of the retained coefficients, 
`r non_zero_distribution[["green"]] %>% formatn` (`r non_zero_distribution_prop[["green"]] %>% formatp`) 
coefficients represent participles that tend to the green word order, and
`r non_zero_distribution[["red"]] %>% formatn` (`r non_zero_distribution_prop[["red"]] %>% formatp`) coefficients represent participles that tend to the red word order (see @fig-coefficient-plots-2). Performing a $\chi^2$ test ($df = `r chi_square_test$parameter`$) on this result, we retrieve a value of $`r chi_square_test$statistic %>% formatd`$ corresponding to a $p$ value $< 0.001$. This indicates that the proportion of the two orders is significantly different from a regular 50-50 split.

From the distribution of the coefficients of both orders (see @fig-coefficient-plots-3), we can also infer that green word order associations are generally stronger, and also feature more extreme logit corrections. Our result is completely in line with the results from both @de_sutter_rood_2005 [247-248] and @bloem_processing_2021 [138]. While @de_sutter_rood_2005 claim that the green order is more lexically specific, @bloem_processing_2021 counters this by claiming that this behaviour is simply the result of a frequency effect:

> The [red] order is more frequent, therefore the threshold is higher for verbs to be significantly
associated with the [red] order -- it requires a higher frequency. Similarly, the
threshold for being significantly associated with the less frequent [green] order is
lower, and more verbs are significantly associated with it. [@bloem_processing_2021, 138]

```{r}
#| label: fig-coefficient-plots
#| column: screen-inset-shaded
#| layout-nrow: 2
#| layout-ncol: 3 
#| fig-subcap:
#|   - "Proportion of removed and retained coefficients"
#|   - "Proportion of preference for either the red or green word" 
#|   - "Distribution of participle coefficients" 
options(scipen=999)

# Retained-removed distribution
row.names(removed_distribution) <- c("retained", "removed")
barplot(removed_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, removed_distribution))))

# Red-green distribution
barplot(non_zero_distribution %>% sort(decreasing=TRUE),
        col=c("green","red"),
        ylim=range(pretty(c(0, non_zero_distribution))))

# Value distribution
boxplot(coefficient ~ order, col=c("green","red"), pch=19, 
        horizontal=T, data=coefficients_non_zero)
```

#### High-level effects and intercept

Recall that we added eight high-level predictors under multifactorial control in order to make sure that we did not lump together variation from other sources into our lexical effects. As was mentioned before, all but one of these high-level predictors were retained by the elastic net regression, which means that they are useful to model the choice between the red and green word order. This is, of course, unsurprising, since we based ourselves on previous research for the selection of these predictors. @tbl-other-coefficients shows an overview of the high-level predictors and their coefficient values. Note that though elastic net regression does not provide significance levels, whether a predictor is retained also reflects the importance of that predictor in the model.

```{r #tbl-other-coefficients}
#| label: tbl-other-coefficients
#| tbl-cap: "Overview of the high-level predictors, their values and whether they were retained by the elastic net regression" 

# All other coefficients start with an underscore
other_coefficients <- coefficients[coefficients$feature %>% substr(1,1) == "_",]
# Change "removed" to "retained"
other_coefficients$retained = ifelse(other_coefficients$removed, "no", "yes")
# Friendly names
other_coefficients$feature <- gsub("_(.+)", "\\1", other_coefficients$feature)
other_coefficients$feature <- gsub("(is|_)", " ", other_coefficients$feature)
other_coefficients$feature <- gsub("(aux) (.+)", "auxiliary '\\2'", other_coefficients$feature)
other_coefficients$predictor <- other_coefficients$feature
other_coefficients$coefficient <- other_coefficients$coefficient %>% formatd(3)
# Only keep interesting columns
other_coefficients <- other_coefficients[, names(other_coefficients) %in% c("coefficient", "predictor", "retained")]
# Re-order
other_coefficients <- other_coefficients[,c(3, 1, 2)]
# Sort
other_coefficients$coefficient <- as.numeric(other_coefficients$coefficient)
other_coefficients <- other_coefficients[order(other_coefficients$coefficient,
                                               decreasing=TRUE),]
# Remove row indices
rownames(other_coefficients) <- NULL
# Output to document
other_coefficients
```

```{r intercept-computation}
intercept <- subset(model_meta$as.data.frame(), predicate == "intercept")[["object"]]
```

Before we discuss the values of the high-level predictors, we look at the intercept of our model and the situation it represents. The intercept for our model has a logit value of `r intercept %>% formatd()`, which means that the "baseline" syntactic realisation is the green order. Keep in mind that our baseline situation represents a situation for (1) a non-separable verb (2) from the perspective of a Belgian speaker (3) in an unedited genre (4) without priming (5) for a verb with a log frequency of zero (6) with an adjectiveness of zero.[^intercept] The corrections stemming from the high-level predictors shift the logit in the directions we know from the literature. If the verb is separable, the logit increases towards the red order. Proclivity to the red order also rises when we shift the perspective to a Netherlandic speaker, when the genre is edited, when the red order is primed and when the auxiliary is *hebben*. We also see that as the log frequency and middle field length increase, so does the logit correction towards the red order, as expected. In the other direction, we see that maximal adjectiveness ($\text{adjectiveness} = 1$) dramatically shifts the red-green choice towards the green order. The *zijn* auxiliary also shifts the coefficients in the direction of the green order. This is again in line with what we would expect.

Only one predictor was not retained: the predictor for auxiliary *worden*. From @bloem_processing_2021 [27], we would expect a slight shift towards the red order for verbal clusters occurring with *worden*, but we see no such shift here. We checked the division between red and green for verbal clusters with *worden*, and it is very comparable to the dataset-wide average (around a 35-65 split in favour of the red order). It seems, then, that in our mixed dataset, *worden* displays very "average" behaviour, and has no particular red preference.

[^intercept]: Distributed predictors with one coefficient per level, in our case the lexical item predictors and auxiliary verb predictors, are not part of the intercept. These predictors can be fully turned off when every level is set to zero. In contrast, when a binary predictor such as `NL` is set to zero, it has another implication: if an attestation does not come from The Netherlands, it is necessarily from Belgium. As such, predictors such as these *are* necessarily a part of the intercept.

#### Comparison to previous research

```{r comparison-load}
source("1-7 Comparison.R")
```

In this section, we will compare our elastic net coefficients to other quantitative measures expressing lexical preference for either the red or green word order. Keep in mind that any comparison is at best coarse, since previous methods did not support multifactorial control. As such, some of the variation ascribed to lexical effects in these older methods will have come from other sources.

```{r comparison-ds-counter}
ds_total_count <- dim(df_ds)[[1]]
ds_zero_count <- dim(ds_zero)[[1]]
ds_zero_perc <- (ds_zero_count / ds_total_count) %>% formatp
ds_na_count <- dim(ds_na)[[1]]
ds_na_perc <- (ds_na_count / ds_total_count) %>% formatp
```

First, we will compare our results to the significant Log Likelihood Ratios (LLRs) found in @de_sutter_rood_2005 [246]. For our comparison, we transformed all LLRs in favour of the green word order to be negative, while LLRs in favour of the red order remained positive. This transformation was made to be able to compare the LLR values, which are all positives, to our elastic net coefficients, the signs of which reflect a pull towards either order. Of the `r ds_total_count %>% formatn` participles with a significant LLR value in the semantic analysis of @de_sutter_rood_2005, `r ds_zero_count %>% formatn` (`r ds_zero_perc`) were eliminated in our elastic net model. This means that the elastic net technique disagrees that a semantic pull exists for these verbs. This difference likely stems from the aforementioned methodological differences in multifactorial control, or overfitting in the older techniques which do not apply cross-validation. An overview of the eliminated coefficients is given in the Appendix.

There are also `r ds_na_count %>% formatn` participles (`r ds_na_perc`) which do not appear in our dataset, and therefore do not have an associated coefficient. Most of these "participles" are actually adjectives, and were likely removed because of our adjectiveness cut-off (see @sec-corpus-querying). An overview of the missing participles is given in the Appendix.

We computed the Pearson correlation between the modified LLR values and our elastic net coefficients and found a significant correlation $r$ of `r ds_cor$estimate %>% formatd(3)` ($p < 0.05$). The correlation shows that there is a fair degree of overlap between the LLR results and our results, though the results are not one to one. Again, it is reasonable to expect a difference in results as a consequence of the differences in methodology, but generally speaking, this correlation shows that our elastic net regression methodology yields comparable results. If we look at the correlation visually (@fig-comparison-ds), we also see that the data points in the top left and bottom right quadrants, i.e. participles which are associated with red in one technique and green in the other and vice versa, are close to the origin and few. This also shows that our results are generally in line with those of @de_sutter_rood_2005.

```{r #fig-comparison-ds}
#| fig-cap: "A visual comparison of the significant LLR values and our elastic net coefficients" 
quadrant_plot(df_ds$llr, df_ds$coefficient, "Log likelihood ratio", "Elastic net coefficient")
```

```{r comparison-bloem-counter}
bloem_total_count <- dim(df_bloem)[[1]]
bloem_zero_count <- dim(bloem_zero)[[1]]
bloem_zero_perc <- (bloem_zero_count / bloem_total_count) %>% formatp
bloem_na_count <- dim(bloem_na)[[1]]
bloem_na_perc <- (bloem_na_count / bloem_total_count) %>% formatp
```

Next, we will compare our elastic net coefficients to the odds ratios (ORs) found in @bloem_processing_2021. We converted the ORs back into (continuous) logits, so we could compare them to our elastic net coefficients. Of the `r bloem_total_count %>% formatn` participles in Bloem's semantic analysis, `r bloem_zero_count %>% formatn` (`r bloem_zero_perc`) were eliminated in our elastic net model. Here again, this means that the elastic net technique finds no semantic pull, again likely due to differences in operationalisation. An sample of the eliminated coefficients is given in the Appendix.

There are also `r bloem_na_count %>% formatn` (`r bloem_na_perc`) participles which do not appear in our dataset, and therefore do not have an associated coefficient. Some of these participles seem to be mis-taggings or noise. We provide a sample in the Appendix.

We computed the Pearson correlation between the converted logits and our elastic net coefficients and found a significant correlation $r$ of `r bloem_cor$estimate %>% formatd(3)` ($p < 0.05$). The correlation again shows an overlap between previous results and our results, despite differences in methodology. If we look at the correlation visually (@fig-comparison-bloem), we see that both models are generally in agreement on whether a participle pulls towards red or green. Note that there are more data points in the top left and bottom right quadrants than in the previous comparison, but this is related to a much larger sample size.

```{r #fig-comparison-bloem}
#| fig-cap: "A visual comparison of the OR values converted to logits and our elastic net coefficients" 
quadrant_plot(df_bloem_nna$logit, df_bloem_nna$coefficient, "Logit", "Elastic net coefficient")
```

## Attaching insights into meaning

While we now have a window to the semantic preferences of participles in the red and green word order, we only see the *surface* of our semantic effects. We do not see the general semantic motivations behind the choice for either word order. We attempted to find semantic patterns in two ways: through distributional semantics on the one hand, and through the use of a lexical database on the other hand.

### Distributional semantics

In an attempt to find semantic patterns, we made use of semantic vectors. Semantic vectors are mathematical vector representations which try to capture the meaning of words [@montes_cloudspotting_2021]. The representations hinge on statistical co-occurrences -- similar words appearing in similar contexts. An important property of semantic vectors for this study is that semantically related or comparable words tend to have similar vectors. This means that, if we can somehow inspect the vector space, we might be able to find semantic "pockets" -- areas of words with similar meanings -- which are conducive to either word order.

As a basis for our semantic analysis, we used the big "COW" vectors for Dutch by @tulkens2016evaluating. These vectors are based on the COW corpus [@schafer-bildhauer-2012-building], a corpus compiled from .be and .nl websites, and were trained using the word2vec algorithm [@mikolov2013efficient]. We chose the COW vectors over vectors trained on other corpora discussed in @tulkens2016evaluating, as the COW vectors performed the best in downstream tasks, and can therefore be assumed to be the richest in semantic information. Note that we did not use the vectors of the participles, but rather those of the associated infinitives. The vectors of infinitives were available for more words, and can also be seen as semantically richer since they appear in more contexts.

To find patterns among the semantic vectors, we attempted to cluster the data using the Partitioning Around Medoids (PAM) technique [@kaufman_partitioning_1990]. PAM selects a predefined number of data points as cluster centres and then builds clusters around those central points. We used the R implementation in the `cluster` package [@maechler_cluster_2022, `pam` function].

PAM requires the number of clusters used to be chosen beforehand. Since we are modelling binary variation, it might seem intuitive to choose two clusters. However, there are two problems with this approach. The first problem is apparent when we attempt to plot the semantic vectors in a lower-dimensional space (see @fig-pam-viz). We see that the semantic space is, simply put, very busy. It is unrealistic to expect clear-cut subdivisions to appear in the data when we impose a structure that is very "global". With this we mean that just two clusters can never capture the variation that is inherent in a semantic space. The second problem stems from a certain methodological naÃ¯vetÃ©. Indeed, it might be naive to think that there is a clear-cut semantic feature that divides all or even most red and green verbs, and that this feature can be easily captured distributionally. This is especially the case given the inherent messiness of semantics.

```{r #fig-pam-viz}
#| fig-cap: A visual representations of the semantic meaning space, dimensionally reduced through PCA
#| fig-width: 5
#| fig-height: 5

pam_df <- read.csv("output/RoodGroenAnthe_coefficients_infused_vectors.csv")

plot_basic_coords <- function(df, technique, do.legend=FALSE) {
  colours <- c("red", "green")

  x_column <- paste0(technique, ".x")
  y_column <- paste0(technique, ".y")

  df <- df[!is.na(df[[x_column]]),] 
  
  par(mar=c(4, 4, 1, 1))
  
  plot(
    df[, x_column],
    df[, y_column],
    pch = 19,
    cex = 0.5,
    col = colours[factor(df$order)],
    xlab = "x",
    ylab = "y",
    asp = 1
  )
  
  if (do.legend) {
    df$order <- factor(df$order, levels=c("red", "green"))

    legend("bottomright",
           legend = levels(df$order),
           pch = 19,
           col = colours,
           bty = "n",
           horiz = FALSE)
  }
}

plot_basic_coords(pam_df, "pca", TRUE)
```

Unfortunately, this insight does not solve our cluster count problem, but merely reformulates it: how many clusters should we use to be able to capture semantic variation at a more local level? We attempted to solve this question in a bottom-up way, using Hartigan's rule [@hartigan1975clustering]. Hartigan's rule is based on the sum of squared distances of points within clusters. We start with two clusters, compute the sum of squared distances in those clusters, add another cluster and compute the sum again. We repeat this iteratively. When the sum of squared distances does not decrease enough to warrant a new cluster, the "ideal" number of clusters is found. As a general rule of thumb, it is advised to keep adding clusters as long as Hartigan's statistic is bigger than ten. We used the `FitKMeans` R function from the `useful` package [@lander_useful_2023] to compute Hartigan's statistic iteratively on all vectors of verbs with a non-zero coefficient. We opted to leave out vectors of verbs with zero coefficients, because we wanted to keep the internal structure of clusters easy to interpret. @tbl-hartigan shows the continual decrease of Hartigan's statistic, until the statistic drops below ten at eight clusters.

```{r #tbl-hartigan}
#| tbl-cap: "Overview of the results of the incremental Hartigan algorithm. When the Hartigan statistic drops below ten, it is no longer beneficial to add another cluster. Clusters are computed on full semantic vectors for all verbs with a non-zero coefficient."

# Load the dataset
hartigan_df <- read.csv("output/hartigan_cluster_search.csv")
hartigan_df <- hartigan_df[order(hartigan_df$Clusters, decreasing=FALSE),]

# Check where to cut display
n_cluster <- hartigan_df %>% filter(!AddCluster) %>% first %>% .$Cluster
hartigan_df <- head(hartigan_df, n_cluster - 1)

# Friendly names
hartigan_df$AddCluster <- ifelse(hartigan_df$AddCluster, "yes", "no")
colnames(hartigan_df) <- c("clusters","Hartigan's statistic","add another cluster?")

hartigan_df
```

Because the semantic vectors are high-dimensional, we cannot plot them easily in two or even three dimensions. As an alternative, we show the internal composition of each of the clusters in @fig-clusters. Additionally, a full line bar, in contrast to a dashed line bar, demonstrates whether a t-test finds the internal distribution in a cluster to be skewed significantly. If there is a significant skew, one could say that cluster has a "dominant" colour. We see that there are several clusters with a green dominant colour, while there is only one with a red dominant colour. The red order is limited to just one cluster. Two clusters do not show a skew towards a specific order and therefore do not have a dominant colour.

```{r load-clusters-r}
#| output: false
source("1-6 Cluster analysis.R")
```

```{r #fig-clusters}
#| fig-cap: An overview of the clusters found in the data. Full line bars indicate a significant skew between red and green verbs in that cluster, in contrast to dashed line bars.
cluster_stats("non_zero.kmeans.full") %>% cluster_plot()
```

Unfortunately, it is difficult to take the semantic vector analysis further from here. Since semantic vectors do not have explainable dimensions, we cannot interpret what semantic properties are characteristic of specific clusters, and therefore of a specific word order. In the next section, we will attempt to get a grip on these semantic properties using a lexical semantic database for Dutch.

### Lexical semantic database

In an attempt to find semantic patterns with explainable dimensions, we turned to Cornetto, a lexical semantic database for Dutch [@piek2013cornetto]. While Cornetto contains only a limited number of verbs found in our sample, it is richly annotated for semantic properties. If we then model our elastic net coefficients by means of these semantic properties, we can gain insights into what aspects of meaning have an influence on the choice between the red and green word order. For this section, we follow the example of @bloem_processing_2021 [132-133], discussed in @sec-semantics-influence. We do this lexical analysis in a post-hoc manner, in contrast to having these lexical properties as predictors in the main regression model, for two reasons. Firstly, the Cornetto database has a limited scope, which would limit our general analysis to fewer verb types. Secondly, our post-hoc analysis is focussed on the lexical meaning only, whereas our attestations in the main analysis are embedded in a sentential context. It would not be appropriate to blindly assign lexical properties to occurrences in possibly much broader contexts.

One feature of Cornetto is that, like in a dictionary, every lemma is annotated for different senses. Unfortunately, our dataset is not tagged for senses, which makes it challenging to look up the semantic attributes of a specific lemma. In an attempt to solve this issue, we turned to the DutchSemCor project [@vossen-etal-2012-dutchsemcor], a corpus of Dutch in which lemmas are tagged for the senses found in Cornetto. We built a frequency table for each verb found in Cornetto, and in this way determined for each verb what its most frequent or "dominant" sense is. We used the semantic properties of those dominant senses in our analysis.

```{r cornetto-stats}
#| output: false

cornetto_df <- read.csv("output/cornetto_info.csv")

cornetto_size <- cornetto_df %>% dim %>% .[1]
cornetto_size_perc <- (cornetto_size / types_count) %>% formatp
cornetto_broad_size <- cornetto_df %>% filter(semantic_type != "") %>% dim %>% .[1]
cornetto_broad_size_perc <- (cornetto_broad_size / types_count) %>% formatp
cornetto_narrow_size <- cornetto_df %>% filter(semantic_feature_set != "") %>% dim %>% .[1]
cornetto_narrow_size_perc <- (cornetto_narrow_size / types_count) %>% formatp
```

Verbs in Cornetto can have two types of semantic tags: broad and narrow semantic tags. A broad semantic tag can be a "process", "state" or "action". A narrow semantic tag is more elaborate and consists of five possible attribute values, which can either be present or not. Ideally, all verbs would be tagged with the most elaborate attributes, but unfortunately this is not the case. Of the `r types_count %>% formatn` types in our dataset, only `r cornetto_size %>% formatn` (`r cornetto_size_perc`) are part of the Cornetto database; `r cornetto_broad_size` (`r cornetto_broad_size_perc`) have a broad semantic tag and `r cornetto_narrow_size` (`r cornetto_narrow_size_perc`) have a narrow semantic tag. We will build analyses with both properties.

```{r speed-stats}
sem_df <- read.csv("output/RoodGroenAnthe_coefficients_semantics_full.csv")
sem_size <- sem_df %>% dim %>% .[1]
sem_size_perc <- (sem_size / types_count) %>% formatp
sem_broad_size <- sem_df %>% filter(semantic_type != "") %>% dim %>% .[1]
sem_broad_size_perc <- (sem_broad_size / types_count) %>% formatp
sem_narrow_size <- sem_df %>% filter(semantic_feature_set != "") %>% dim %>% .[1]
sem_narrow_size_perc <- (sem_narrow_size / types_count) %>% formatp
```

We further expanded the semantic properties from Cornetto with the emotion assessment ratings from @speed_ratings_2023. We chose to use this dataset instead of an automated system as the dataset is large, based on actual human judgements and of a very high quality. We chose to use a simple dichotomy between "neutral" and "valenced" verbs instead of a further distinction between "positive" and "negative" in order to build a model that is more straightforward to interpret. After intersecting the verbs available in Cornetto and in the @speed_ratings_2023 dataset, we were left with `r sem_size` verbs (`r sem_size_perc`) of which `r sem_broad_size` (`r sem_broad_size_perc`) have a broad semantic tag and `r sem_narrow_size` (`r sem_narrow_size_perc`) have a narrow semantic tag. Following the example of @bloem_processing_2021, we also added valency information to the analyses. We did not choose to also include transitivity, as valency and transitivity are related, and we wanted to avoid interference in the analyses. An overview of all variables is given in @tbl-variables.

|variable|description|
|---|---|
|control|the subject of the verb is capable of acting with volition|
|attributive|the verb expresses a relation of ownership|
|spatial|the verb expresses a location or movement of (one of) the participants(s)|
|cognition|the verb demands emotional, perceptual or mental activity|
|dynamic|the verb expresses a non-static, changing situation|
|valency|the number of arguments of the verb|
|valence category|whether the verb is rated as "neutral" or "valenced"|

: An overview of the variables of our analyses. Semantic descriptions taken from @piek2013cornetto. {#tbl-variables}

As mentioned before, only a subset of verbs is tagged for the most specific semantic properties. Therefore, we made two analyses. In the fine-grained analysis, we used the smallest dataset with the richest annotations, while in the coarse-grained analysis, we traded the five semantic properties for the three-way variable "semantic type" so we could use the larger dataset. We included the second, coarser analysis to be able to better assess the influence of variables with weaker effects, which would not attain significance at a smaller dataset size.

```{r regression-fit-format}
format_num_col <- function(p_value) {
  if (p_value < 0.01) {
    return("<0.01")
  } else {
    return(formatd(p_value, 4))
  }
}
format_num_col <- Vectorize(format_num_col)

format_table <- function(df) {
  df <- df %>% tidy %>% 
    tidy_categorical(m = df) %>% 
    filter(!is.na(term)) %>% 
    select(-c(reference, term, effect)) %>%
    mutate(variable = as.character(variable),
           level = as.character(level)) %>%
    mutate(level = ifelse(variable == level, "", level),
           variable = as.character(variable)) %>% 
    #group_by(variable) %>%
    #mutate(variable = ifelse(row_number() == 1, variable, "")) %>%
    #ungroup() %>%
    mutate(term = ifelse(level != "", paste0(variable, ": ", level), variable)) %>%
    select(-c(variable, level))
  df <- df[, c("term", "estimate", "std.error", "statistic", "p.value")]

  df$sig <- ifelse(df$p.value <= 0.05, "*", "")
  
  df$estimate <- df$estimate %>% formatd(2)
  df$std.error <- df$std.error %>% formatd(2)
  df$statistic <- df$statistic %>% formatd(2)
  df$p.value <- df$p.value %>% format_num_col()

  colnames(df) <- c("term", "coefficient", "std. error", "t value", "p value", " ")

  return(df)
}

format_posthoc_table <- function(df) {
  df <- df$contrasts %>% tidy()
  df$term <- NULL
  df$df <- NULL
  df$null <- NULL
  df$null.value <- NULL
  df$contrast <- gsub("[()]", "", df$contrast)
  df$contrast <- gsub("-", "â†”", df$contrast)

  df$sig <- ifelse(df$adj.p.value <= 0.05, "*", "")

  df$estimate <- df$estimate %>% formatd()
  df$std.error <- df$std.error %>% formatd()
  df$statistic <- df$statistic %>% formatd()
  df$adj.p.value <- df$adj.p.value %>% format_num_col()

  colnames(df) <- c("contrast", "coefficient", "std. error", "t value", "p value", "")

  return(df)
}
```

```{r semset-regression}
#| output: false

# Logical order
sem_df$valency <- factor(sem_df$valency, levels=c("mono", "di", "tri"))
sem_df$ValenceCategory <- factor(sem_df$ValenceCategory,
                             levels=c("neutral", "positive", "negative"))
sem_df$valence <- sem_df$ValenceVsNeutral %>% factor(levels=c("neutral", "valenced"))
sem_df$semantic_type <- factor(sem_df$semantic_type, levels=c("state", "process", "action"))

# Not all words in Cornetto have a semantic feature set assigned to them
# So I have to filter them out
df_semset <- sem_df %>% filter(semantic_feature_set != "")

semset_fit <- lm(
  coefficient ~ control + attributive + spatial + cognitive + dynamic + valency + valence,
  data = df_semset
)
semset_fit_tidy <- semset_fit %>% tidy()
```

In our fine-grained analysis, based on the smaller dataset, we modelled the elastic net coefficients of all verbs using the variables from @tbl-variables as predictors in a linear regression model. @tbl-semset-regression shows the results of this regression model. Keep in mind that the results need to be interpreted with regards to the reference situation expressed as the "Intercept". In our case, this reference situation is:

- a verb that is not tagged for "control", "attributive", "spatial", "cognitive" or "dynamic";
- that is intransitive ("monovalent");
- that has a neutral valence.

Our reference situation expresses a slight bias towards the red order.

The regression results show the differences to the elastic net coefficient of a verb when one of these elements changes. We see that for verbs which are valenced (either positive or negative), there needs to be a small adjustment towards the green word order (`r semset_fit_tidy %>% filter(term == "valencevalenced") %>% .$estimate %>% formatd(3)`). Of all semantic properties, we see that the "cognitive" tag reaches closest to significance, though it does not attain it.

```{r tbl-semset-regresion}
#| label: tbl-semset-regression
#| tbl-cap: "Results of the fine-grained regression analysis. * indicates statistical significance"
semset_fit %>% format_table()
```

For our coarse-grained analysis, based on the larger database, we again modelled the elastic net coefficients for the different verbs, but this time using the simpler, more coarse semantic labelling. In fact, we simplified the semantic labelling to be binary, collapsing the "process" and "action" labels into a new label, "no state". We made this change in order to improve data balance and to make the predictor align more with the stative-dynamic dichotomy by @pardoen1991interpretatie (see @sec-semantics-influence).  @tbl-semtype-regression shows the results of this model. The intercept situation here represents a reference situation for:

- a verb that expresses a state;
- that is intransitive ("monovalent");
- that has a neutral valence.

```{r tbl-semtype-regresion}
#| label: tbl-semtype-regression
#| tbl-cap: "Results of the coarse-grained regression analysis. * indicates statistical significance"
df_semtype <- sem_df %>% filter(semantic_type != "")

# Some relevelling
df_semtype$semantic_type <- df_semtype$semantic_type %>% 
  fct_collapse(state = c("state"), no_state = c("process", "action"))
df_semtype$valence <- df_semtype$ValenceVsNeutral %>% factor(levels=c("neutral", "valenced"))
#df_semtype$valence <- df_semtype$ValenceCategory %>% 
#  fct_collapse(neutral = c("neutral"), emotional = c("positive", "negative"))

semtype_fit <- lm(
  coefficient ~ valency + valence + semantic_type, data=df_semtype
)
semtype_fit_tidy <- semtype_fit %>% tidy()
semtype_fit %>% format_table()
```

With this larger dataset size, we see that semantics- and valency-related predictors now reach significance. We will treat these differences one by one.

1. Valency

Starting from an intransitive reference point, changes to transitive or ditransitive verbs require a correction towards the green order (`r semtype_fit_tidy %>% filter(term == "valencydi") %>% .$estimate %>% formatd(3)` and `r semtype_fit_tidy %>% filter(term == "valencytri") %>% .$estimate %>% formatd(3)` respectively). The post-hoc analysis [through `emmeans`, @emmeans] in @tbl-valency-posthoc shows that there is only a difference between intransitive and both transitive and ditransitive verbs. There are no mutual differences between transitive and ditransitive verbs.

```{r #tbl-valency-posthoc}
#| tbl-cap: Results of the `emmeans` posthoc analysis for valency. * indicates statistical significance
posthoc_valency <- emmeans(semtype_fit,
                   revpairwise ~ valency,
                   type="response",
                   weights="proportional")
format_posthoc_table(posthoc_valency)
```

2. Valence category

Like in the fine-grained analysis, starting from a neutral reference point, verbs with an emotional load (either positive or negative) lead to a slight shift towards the green word order (`r semtype_fit_tidy %>% filter(term == "valencevalenced") %>% .$estimate %>% formatd(3)`).

3. Semantic type

Our reference situation expresses a "state" semantic type. We see that there is a significant difference between this state reference type and the "no state" type consisting of both "action" and "process". We see that when the semantic type changes to not a state, the elastic net coefficients shift slightly towards the green order (`r semtype_fit_tidy %>% filter(term == "semantic_typeno_state") %>% .$estimate %>% formatd(3)`).

### Semantic conclusions 

In the previous sections, we attempted to find patterns in the coefficients computed by the elastic net technique (see @sec-computing-semantic-pull). The main objective was to find whether there are certain correlations between the semantic pull towards either the red or green word order in Dutch and a specific semantic property. Previous theories of semantic influence on red and green have posited that the green word order is more lexically specific [@de_sutter_rood_2005], that the green order is more likely to lead to a stative interpretation [@pardoen1991interpretatie] and that the green word order is more associated with cognitive and negative verbs [@bloem_processing_2021]. In this section, we will examine these theories in light of our own results.

First of all, our clusters of verbs, based on a bottom-up clustering of semantic vectors, shows that there are more semantic areas in our semantic space which are predominantly green. While it is true that our results contain more green coefficients in the first place, the split between predominantly red and green clusters in our bottom-up clusters is much more skewed than the split between red and green verb types (see @sec-results-lexical). On the basis of this outcome, one could say that this result corroborates the idea by @de_sutter_rood_2005 that the green word order is more lexically specific, since it clearly captures a larger proportion of the semantic space. We also saw this in the range of coefficients in general in @fig-coefficient-plots-3.

Based on the results of the analyses of the Cornetto lexical semantic database, we can also say that the choice between the red and green word order is at least partially syntactically and semantically motivated. We were not able to confirm the finding by @bloem_processing_2021 that verbs with a cognitive dimension tend towards the green word order, though this is likely a consequence of the dataset size. Both the fine-grained and coarse-grained analysis did show a skew towards green with verbs with an emotional load, however, which is comparable to the pattern found by @bloem_processing_2021 with predominantly negative verbs.

In the same coarse-grained semantic analysis, we also saw that there is a slight yet significant difference between "state" semantic types and "non-state" semantic types: non-states are found to edge slightly towards the green order. This finding is peculiar in light of the long-standing idea launched by @pardoen1991interpretatie that the green order would facilitate the stative interpretation of a verb cluster. However, we already know from @de_sutter_rood_2005 that this idea should be detached from the actual semantics of a verb, and we see here that this is indeed seems to be the case.

Syntactically, the coarse-grained semantic analysis shows there to be a difference, though another slight one, between intransitive verbs on the one hand and transitive and ditransitive verbs on the other hand. Both transitive and ditransitive verbs shift towards the green word order slightly compared to intransitive verbs. This is surprising, since we would expect the middle field to be shorter (and therefore more conducive to green word orders) with intransitive verbs as they do not select any objects, in contrast to (di-)transitive verbs. Perhaps it is cognitively more accommodating to express the subject and the main verb contiguously, which is only possible with intransitive verbs.

## Discussion

In this article, we built a model which attempts to explain the variation between red (auxiliary + participle) and green (participle + auxiliary) verbal clusters in Dutch in order to demonstrate the use of elastic net regression. We presented elastic net regression as a means to build fairer models of morphosyntactic variation, as it allows lexical items to appear on the same level as traditional, high-level predictors, in contrast to other statistical techniques. In addition, the technique can disable certain levels of lexical item predictors if necessary to boost the quality of the model, which makes it robust against typical Zipfian-related sparsity issues, and prevents overfitting.

We extracted `r nrow(df_filtered) %>% formatn` verb clusters from the SoNaR corpus [@oostdijk_d-coi_2008] and encoded the most important high-level predictors for each attestation to include in the analysis. We also included the lexical type of each verb cluster in the model as a full predictor, which is the most important innovation in our model. We found that our model yields similar results for both the high-level predictors as well as the lexical associations compared to previous models, which shows that the model serves its intended purpose of delivering reliable results for predictors of different natures in a single model. Since lexical items can be part of the fixed effects structure in elastic net regression, they now also come under multifactorial control. As such, our values for morphosyntactic pull towards either variant should be more free from influence from unexplained variance, unlike in other techniques. At the same time, our holistic model proves that, even when high-level predictors are taken into account *in the same model*, there is a lot of variation left to be explained on lexical terms. It seems desirable for our understanding of morphosyntactic variation, then, to include lexical items in future morphosyntactic analyses by default.

While our model shows that lexical influence exists, we do not have an all-encompassing explanation for all lexical patterns we see in our specific case study. Our verb clusters show that the green word order might be more lexically specific, and through our analysis of a lexical semantic database, we found differences between "state" and "non-state" semantic types. However, this effect, like the other effects, is small, and does not account for the broad range of semantic pull values we see. Perhaps many of the lexical patterns we find are too local to appear in our analysis, or we need to find other possible explanations of the variation we see, i.e. phonological similarity.

For future research, it would be interesting to apply the elastic net technique to other morphosyntactic alternations, especially in other languages. Steps in this regard have already been taken [@engel_constraints]. In this way, we will learn more about how language users choose between equivalent variants, and how different influences relate to each other.

## Appendix {#appendix .appendix}

### Try it yourself

- Misschien ook een aansporing om zelf naar de coÃ«fficiÃ«nten te kijken met Rekker â€” met automatische link

### Corpus and querying {#sec-corpus-querying}

To compute the semantic pull of the different verbs in the red and green word order, we collected all red and green verb clusters in subordinate clauses in the SoNaR Corpus [@oostdijk_d-coi_2008] and SoNaR New Media Corpus [@oostdijk_sonar_2014]. Since we are interested in syntactic alternances, we needed a syntactically informed corpus format ("treebank") in order to reliably find the attestations we need. While the SoNaR corpus does not ship with syntactic information, much of the corpus material from SoNaR is also included in the Lassy corpus [@van_noord_large_2013], which *is* syntactically annotated using Alpino [@van_noord_at_2006]. We retrieved the syntactic information from SoNaR available in Lassy, and parsed the remaining sentences using Alpino ourselves. This left us with a fully syntactically informed SoNaR corpus, ready to be queried for red and green word orders.

```{r}
df_original <- read.csv("RoodGroenAnthe.csv")
```

In order to query the syntactic information of the entire SoNaR corpus, we used mattenklopper [@sevenants_mattenklopper_2023], a treebank search engine tailor-made for this study. While there are several Alpino search engines available, many of which are much more user friendly *and* faster than the custom search engine used here [e.g. GrETEL, @augustinus_example-based_2012; PaQu, @kleiweg_paqu_2023], these engines all have specific problems which made it so they could not be used for this study. In both GrETEL and PaQu, it is only possible to retrieve entire sentences. One cannot further retrieve the participles or auxiliaries in a verb cluster -- this must be done manually. GrETEL only supports searching through subsections of SoNaR[^gretel] and PaQu does not even offer the SoNaR corpus for querying. Finally, GrETEL results are limited to only 500 sentences due to copyright concerns, which is not enough for a sophisticated analysis. The custom mattenklopper engine was developed as a solution to all these problems. It is available online and can be used for future alternance studies of Dutch using Alpino-based corpora. The xpath queries used to search the corpus are included in the appendix. The mattenklopper search engine returned `r nrow(df_original) %>% formatn` attestations of either the red or green word order.

[^gretel]: In addition, the SoNaR corpus included in the GrETEL web interface is parsed using an older version of Alpino, which results in unreliable annotations.


### Filtering and enriching

```{r}
filtering_numbers <- read.csv("output/filtering_numbers.csv")
```

The results were further filtered in order to guarantee the quality of the attestations. In short, duplicates were removed, tokenisation errors were fixed (i.e. removing superfluous punctuation from participles) and obvious tagging mistakes were removed (e.g. words such as *zgn* 'so-called' and *gemiddeld* 'average' were removed). In addition, wrong participle endings (e.g. *gebeurt* instead of *gebeurd* 'happened') were corrected using *naive-dt-fix* [@sevenants_naive-dt-fix_2023], a library for the R language designed for this study. This library automatically corrects wrong participle endings by relying on the relative frequencies of all possible spellings. The most frequent spelling is seen as the correct spelling and is used as a correction.[^dt] Declensed words were also removed (e.g. *geplaatst**e***). Past participles cannot be declensed in Dutch, so all declensed forms in the corpus tagged as participles are, in fact, mis-tagged adjectives. We also removed all verb clusters with an auxiliary other than *hebben*, *zijn* or *worden* and removed all attestations without a sentence ID (which we need to compute priming). In addition, all types occurring less than 10 times were removed in order to guarantee a stable estimate for the semantic pulls of each type. As a result of these operations, `r subset(filtering_numbers, name == "first_removal")[1,][["no_items"]] %>% formatn` attestations were removed.

[^dt]: Some participles are so often misspelled that they are seen as the correct spelling by the program. These cases are overruled by a built-in list of "problematic" participles which have their correct spellings enforced.

Furthermore, the attestations were enriched with additional information to be used in the multifactorial elastic net regression. Firstly, regional information was added for each attestation. SoNaR comes with contextual information about its documents, such as country of origin information. Since region is an important influence in the red-green word order, this variable is vital for multifactorial control.

We also used the subcorpus division in SoNaR (e.g. `WR-P-E-A_discussion_lists`, `WR-P-E-F_press_releases`) to distinguish between edited and unedited genres. We decided to focus on an edited-unedited dichotomy, because it is difficult to assess the formality of certain genres in the corpus (e.g. websites and blogs). By focussing on whether a genre is typically edited or not, we sidestep these issues, but we are still able to include some form of formality distinction. Refer to @tbl-sonar-editing for an overview of our judgements.

| subcorpus | contents | degree of editing |
|---|---|---|
| WR-P-E-A | discussion lists | unedited |
| WR-P-E-C | e-magazines | edited |
| WR-P-E-E | newsletters | no attestations[^no-attestations] |
| WR-P-E-F | press releases | edited |
| WR-P-E-G | subtitles | edited |
| WR-P-E-H | teletext pages | edited |
| WR-P-E-I | web sites | edited |
| WR-P-E-J | wikipedia | edited |
| WR-P-E-K | blogs | edited |
| WR-P-P-B | books | edited |
| WR-P-P-C | brochures | edited |
| WR-P-P-D | newsletters | edited |
| WR-P-P-E | guides manuals | edited |
| WR-P-P-F | legal texts | edited |
| WR-P-P-G | newspapers | edited |
| WR-P-P-H | periodicals magazines | edited |
| WR-P-P-I | policy documents | edited |
| WR-P-P-J | proceedings | edited |
| WR-P-P-K | reports | edited |
| WR-U-E-E | written assignments | edited |
| WS-U-E-A | auto cues | edited |
| WS-U-T-B | texts for the visually impaired | edited |
| WR-P-E-L | tweets | unedited |
| WR-U-E-A | chats | unedited |
| WR-U-E-D | sms | unedited |

: An overview of the SoNaR subcorpora and our edited-unedited judgement {#tbl-sonar-editing}

[^no-attestations]: The newsletters subcorpus is incredibly small, hence why we have no attestations.

Adjectiveness information was added for all participles. Adjectiveness is expressed as a ratio denoting how often a participle functions as an adjective in language use:
$$
\frac{\text{\#uses as an adjective}}{\text{\#uses as an adjective + \#uses as a participle}}
$$

$0$ denotes no adjectival use, $1$ denotes maximal adjectival use. We computed adjectiveness on the entire Lassy corpus [@van_noord_large_2013].[^adjectiveness-dataset]

[^adjectiveness-dataset]: The adjectiveness dataset is published as a separate dataset for re-use by other researchers. See @sevenants_adjectiveness_2023.

Because the Alpino syntactic parser marks separable verbs by infixing an underscore (`_`) between the preposition and verb root, we can exploit this behaviour to automatically infer whether a verb cluster contains a separable verb.

To compute the length of the middle field, we calculated the number of words between the start of the clause and the verbal cluster itself. This information is based on the tokenisation of the SoNaR corpus.

We included frequency information from the SUBTLEX dataset [@keuleers_subtlex-nl_2010] in order be able to assess the effect of frequency. Because frequency is typically Zipfian [@zipf_psycho-biology_1965], we transformed the frequency information using the natural logarithm for a multitude of reasons: (i) to compress the frequency variation among the types in our dataset (ii) to make the distribution of priming more normal (iii) because it makes the distribution more psychologically real.

Priming information is also important to include. To obtain priming information, we relied on the sentence IDs included in the SoNaR corpus. Consider the following example:

::: {.center}
WR-P-P-B-0000000103.p.37.s.4
:::


The ID refers to document 103 of the `WR-P-P-B` component of the SoNaR corpus ("books"). Within that document, it refers to the 4^th^ sentence of the 37^th^ paragraph. The window we chose for priming is one paragraph: this means that in our example, we would consider all attestations from paragraph 36 and all sentences leading up to sentence 4 of paragraph 37 to be possible prime sources. It was not possible to work on the sentence level, since paragraphs can have a variable number of sentences and not all sentences have red-green attestations in the dataset.

We included priming in our model by using a corrected log-odds measure, which we will call the "priming ratio". For every attestation, we computed the following equation:
$$
\ln \left(\frac{\text{\#red primes} + 0.001}{\text{\#green primes} + 0.001}\right)
$$
We computed the ratio between the number of red and green primes and used Laplace smoothing [@brysbaert_dealing_2013] to prevent division by zero. The natural logarithm attenuates large disparities between red and green *and* turns our priming ratio into a continuous variable ranging from $-\infty$ to $+\infty$.

As a final step, we removed all participles for which no adjectiveness value was defined, as these were found *not* to be participles but mis-taggings. For the same reason, participles with an adjectiveness value of over 0.9 were removed. In addition, all attestations for which no region information was defined were also removed, because they lack the information required for the multifactorial analysis. As a result of these two steps, another `r subset(filtering_numbers, name == "second_removal")[1,][["no_items"]] %>% formatn` items were removed.

#### Converting the dataset

To compute the semantic preference of the participles found in our attestations, we used elastic net regression, the technique detailed in @sec-elastic-net-regression. In contrast to regular regression techniques, a tabular dataset cannot be used "as-is" for analysis with elastic net. Instead, the dataset has to be supplied in a matrix form. Consider the following toy example:

| Word order | Participle | Country | Adjectiveness |
|---|---|---|---|
| green | gebroken | Belgium | 0.5 |
| red | mislukt | The Netherlands | 0.4 |
| green | gebeurd | Belgium | 0.1 |

In the matrix form, each multidimensional column is converted so that each unique value of that column becomes its own predictor. In our case, all unique values of the column `Participle` will become binary predictors, each predictor indicating whether that participle occurs in the verb cluster or not. This means our matrix will be inherently sparse, since each verbal cluster can only feature one participle. Binary predictors such as `Country` are also converted to a binary column in the matrix, and simply indicate a deviation from the reference level. For example, if `is_BE` is a binary column, a Belgian attestation will be encoded as `1`, and a Netherlandic attestation as `0`. The `Adjectiveness` column is numeric and can be adopted as-is. The response variable `Word order` is also encoded as a binary variable, as is typical in logistic regression, but it is not a part of the input matrix. The input matrix for our toy example would look as follows:

| is_gebroken | is_mislukt | is_gebeurd | is_BE | adjectiveness |
|---|---|---|---|---|
| 1 | 0 | 0 | 1 | 0.5 |
| 0 | 1 | 0 | 0 | 0.4 |
| 0 | 0 | 1 | 1 | 0.1 |

The response variables would be encoded as [ 0, 1, 0 ] with the red order as the reference level.

To facilitate the conversion process, we used *ElasticToolsR* [@sevenants_elastictoolsr_2023], an R library written for this study. It can automatically convert "traditional" datasets to the matrix format detailed above in seconds.

### Comparison tables

```{r #tbl-comparison-ds-zero}
#| label: tbl-comparison-ds-zero
#| tbl-cap: "Overview of all participles which have significant LLR values, but were eliminated in our elastic net regression model" 
ds_zero <- ds_zero[c("feature","coefficient","llr")]
# Re-order
ds_zero <- ds_zero[,c(1, 3, 2)]
ds_zero <- ds_zero[order(ds_zero$llr, decreasing=FALSE),]
colnames(ds_zero) <- c("participle", "LLR", "elastic net coefficient")
row.names(ds_zero) <- NULL
ds_zero
```

```{r #tbl-comparison-ds-na}
#| label: tbl-comparison-ds-na
#| tbl-cap: "Overview of all participles which have significant LLR values, but do not appear in our dataset and therefore do not have coefficients" 
ds_na <- ds_na[c("feature","coefficient","llr")]
ds_na$coefficient <- "none"
# Re-order
ds_na <- ds_na[,c(1, 3, 2)]
ds_na <- ds_na[order(ds_na$llr, decreasing=FALSE),]
colnames(ds_na) <- c("participle", "LLR", "elastic net coefficient")
row.names(ds_na) <- NULL
ds_na
```

```{r #tbl-comparison-bloem-zero}
#| tbl-cap: "Sample of all participles which have OR values, but were eliminated in our elastic net regression model. ORs converted to logits" 
bloem_zero$abs_coeff <- abs(bloem_zero$coefficient)
bloem_zero <- bloem_zero[order(bloem_zero$abs_coeff, decreasing=TRUE),]
bloem_zero <- head(bloem_zero, 8)
bloem_zero <- bloem_zero[c("feature","coefficient","logit")]
# Re-order
bloem_zero <- bloem_zero[,c(1, 3, 2)]
bloem_zero <- bloem_zero[order(bloem_zero$logit, decreasing=FALSE),]
colnames(bloem_zero) <- c("participle", "logit", "elastic net coefficient")
row.names(bloem_zero) <- NULL
bloem_zero
```

```{r #tbl-comparison-bloem-na}
#| label: tbl-comparison-bloem-na
#| tbl-cap: "Sample of all participles which have logit values, but do not appear in our dataset and therefore do not have coefficients. ORs converted to logits"
bloem_na$abs_coeff <- abs(bloem_na$coefficient)
bloem_na <- bloem_na[order(bloem_na$abs_coeff, decreasing=TRUE),]
bloem_na <- head(bloem_na, 8)
bloem_na <- bloem_na[c("lemma","coefficient","logit")]
bloem_na$coefficient <- "none"
# Re-order
bloem_na <- bloem_na[,c(1, 3, 2)]
bloem_na <- bloem_na[order(bloem_na$logit, decreasing=FALSE),]
colnames(bloem_na) <- c("lemma", "logit", "elastic net coefficient")
row.names(bloem_na) <- NULL
bloem_na
```

### Xpath queries

**Xpath queries for identifying eligible clauses**

Red order:

```{.r .code-overflow-wrap}
//node[(@cat="cp" or @cat="rel" or @cat="inf") and //node[(@wvorm="pv" or @wvorm="inf") and @begin < ./preceding-sibling::node/node[@wvorm="vd"]/@begin | ./following-sibling::node/node[@wvorm="vd"]/@begin]]
```

Green order:

```{.r .code-overflow-wrap}
//node[(@cat="cp" or @cat="rel" or @cat="inf") and //node[(@wvorm="pv" or @wvorm="inf") and @begin > ./preceding-sibling::node/node[@wvorm="vd"]/@begin | ./following-sibling::node/node[@wvorm="vd"]/@begin]]
```

**Xpath queries for retrieving verb cluster participle**

```{.r .code-overflow-wrap}
.//node[@rel="hd" and @wvorm="vd" and @begin $SIGN$ ../../node[@rel="hd" and @pt="ww"]/@begin and not(../../@cat="smain") and ../../../../node[@id="$ID$"]]
```

with `$SIGN` = `>` for the red order, `<` for the green order, and `$ID` = the ID of the parent sentence

**Xpath queries for retrieving verb cluster auxiliary**

```{.r .code-overflow-wrap}
.//node[@rel="hd" and @pt="ww" and @begin $SIGN$ ../node/node[@rel="hd" and @wvorm="vd"]/@begin and not(../@cat="smain") and ../../../node[@id="$ID$"]]
```

with `$SIGN` = `<` for the red order, `>` for the green order, and `$ID` = the ID of the parent sentence