---
title: "Investigating lexical-semantic effects on morphosyntactic variation using elastic net regression and generalised additive models"
author:
  - name: Anthe Sevenants
    email: anthe.sevenants@kuleuven.be
    orcid: 0000-0002-5055-770X
    affiliations:
      - name: KU Leuven
  - name: Freek Van de Velde
    email: freek.vandevelde@kuleuven.be
    orcid: 0000-0003-3050-2207
    affiliations:
      - name: KU Leuven
  - name: Dirk Speelman
    email: dirk.speelman@kuleuven.be
    orcid: 0000-0003-1561-1851
    affiliations:
      - name: KU Leuven
format:
  html:
    toc: true
  docx:
    toc: false
editor: visual
title-block-banner: true
bibliography: references.bib
toc: true
toc-depth: 4
toc-location: left
tbl-cap-location: bottom
fig-cap-location: bottom
number-sections: true
reference-location: margin
csl: chicago-author-date.csl
df-print: kable
execute:
  echo: false
---

```{css, echo = FALSE}
.center {
  text-align: center !important
}
```

```{r initialisation}
#| output: false
library(scales)
library(magrittr)
library(glossr)
use_glossr()

formatn <- function(number) {
    format(round(as.numeric(number), 1), nsmall=0, big.mark=",")
}

formatd <- function(number, nsmall=2) {
    format(round(number, nsmall), nsmall=nsmall)
}

formatp <- function(number) {
    label_percent()(number)
}
```

## Introduction

Lexical semantics have played an important role in linguistics for a long time. In very formal theories of language, the lexicon is assigned a special position. An example of this is found in Syntactic Theory [@sag_syntactic_2003], where the lexicon is specifically acknowledged in the theory of syntax. Indeed, the authors mention (131):

> Trying to do syntax without acknowledging the associated semantic regularities would lead to missing many fundamental generalizations about linguistic structure.

In this view the lexicon is seen as a distinct module, separated from syntax. Construction grammar goes one step further and forfeits the border between lexis and syntax entirely. Lexical material and syntactic constraints appear on the same level and are mixed together in a "lexicon-syntax continuum" [@hoffmann_construction_2013, 1]. This is by design, since the construction grammar architecture can "account for the entirety of language instead of making a distinction between core linguistics \[...\] and \[...\] exceptions" [@beuls_computational_2016 5]. In such an account, lexical effects belong to the core of grammar. While impressive efforts have been made to investigate lexical-semantic effects in the wide spectrum of theoretical persuasions, methodological innovation has lagged behind. This is in part due to the fact that it is quite hard to research semantic effects quantitatively.

One area in which lexical effects are important is the domain of morphosyntactic alternances: the idea that the lexical material in a sentence influences which construction is used. Currently, there are two ways in which lexical-semantic effects on morphosyntactic variation is measured. The first way is through annotations of one specific semantic feature. For example, in @nevalainen_probabilistic_2008, attestations of the *of*- or *s*-genitive are coded for animacy of the possessor. There are three problems with this approach. The first is that this way of measuring semantic effects only highlights one aspect of the semantic value of lexical items. In addition, one can typically only measure the *general* effect of this semantic feature; there are no per-word results. Finally, since annotations hinge on semantic estimations by humans, or are limited to the perception of meaning by humans, this could induce subjective bias, necessitating the need for intercoder reliability or other tools to ensure reproducibility of the effects. Such tools can be cumbersome, time-consuming or still leave a measure of uncertainty.

The second way in which semantic effects on morphosyntactic alternances are measured is by analysing the general semantic 'pull' of lexical items towards the opposing constructions, i.e. by measuring the frequency distribution of the lexical items. An early example is @tummers_quantifying_2004, who use the log-likelihood ratio in a regression analysis to investigate the lexical specificity of opposing constructions. One option explored by @bloem_processing_2021 is to investigate the residuals of each data point in a logistic regression. The idea is that the mean residuals for a specific type (e.g. the residuals for all data points with the same word) hint at the lexical association of that type towards an option of a specific alternance [@bloem_processing_2021 118, 135]. While this approach to measuring lexical-semantic effects has been shown to work, it lumps together unexplained variation and lexical-semantic effects into one category, which is statistically infelicitous.

Another traditional option would be to apply collostructional analysis in order to investigate the lexical pulls on a syntactic alternance [e.g. @stefanowitsch_collostructional_2013]. While this technique works and indeed gives us the lexical pulls of each participle individually, it does not feature multifactorial control. This means it is difficult to focus purely on the lexical semantic effects, as there can be interference from other (un)known factors.

To retain this multifactorial control, a current state-of-the-art analysis of lexical effects would attempt to use random effects in a mixed-effects logistic regression [cf. @gries_most_2015]. In such an approach, specific lexical verbs partaking in an alternance are seen as levels of a factor for which one extra parameter is fitted, as a random factor. This would result in a correction term for each type separately, which would indicate how specific words contribute towards the choice between two constructions. However, as @van_de_velde_investigating_2019 [168-169] describe, there are generally two issues with this approach. The first issue is that using random effects in this way is, strictly speaking, conceptually unsound. Random effects are meant to account for structural associations in the data, to avoid Type I errors. If one were to use the by-verb corrections from the random factor as a variable of interest, to boost the statistical power of the analysis (avoiding Type II errors), this would amount to making one's model less conservative, letting Type I errors sneak in via the backdoor. If the lexical verbs are a variable of interest, one should make them part of the fixed effects.

This brings us to the second issue raised by @van_de_velde_investigating_2019; there are *many* possible lexical types, and word distribution is notoriously skewed [@zipf_psycho-biology_1965]. As @bloem_processing_2021 [117] says, this results in "an impossible task when there are many unique \[words\] -- you can't create a line from a single data point" -- and models will fail to converge given too many lexical types. A possible solution to this problem of high dimensionality would be to use some sort of 'semantic binning', i.e. by grouping individual words into larger semantic categories. However, this means we lose the ability to look at lexical effects at the level of individual words, and also introduces bias into the analysis at the level of the grouping itself, unless you use clusters, like in @levshina_radically_2014 or @van_de_velde_investigating_2019.

In this article, we demonstrate a next-generation methodology for researching lexical-semantic effects on morphosyntactic alternances through the use of elastic net regression. The results of this technique will be combined with semantic vectors, clustering algorithms and General Additive Models (GAMs) in order to gain more general semantic insights. First, however, we will explain what the Elastic Net technique does and why it is so useful for lexical-semantic research.

## Elastic net regression {#sec-elastic-net-regression}

Elastic net regression is a regression analysis technique which incorporates so-called 'regularisation' into its model fitting procedure. In contrast to 'normal' regression techniques like linear or logistic regression, elastic net regression adds a penalisation factor to the maximum likelihood procedure used to compute the model parameters. In practice, this means that the model coefficients will be 'shrunk' in order to guarantee fairer and more generalisable predictions, avoiding overfitting. By additionally using cross-validation (see below), we can fit many more variables than using traditional regression, even to the extent that we can have a dataset with $n$ observations and $p$ variables, where $n < p$. For a semantic analysis with potentially hundreds of words as predictors, this becomes a powerful technique. Additional variables can still be added to the analysis, which means the analysis can remain under multifactorial control.

In addition, model coefficients can be shrunk to the extent that they are reduced to zero, effectively disabling the predictors of those coefficients outright. This behaviour is very welcome in high-dimensional situations, like when entering hundreds of words as predictors for a lexical-semantic analysis. This allows us to effectively carry out variable selection *within* the regression analysis itself.

### Components of elastic net regression

When we speak of elastic net regression, we actually mean the combination of 'lasso regression' and 'ridge regression'. Both are penalised regression techniques, but their exact operationalisations are different.

Lasso regression attenuates predictions through the use of the so-called L1-norm. In lasso regression's objective function, the coefficients of the model are optimised for maximum likelihood, like in a traditional regression fitting procedure. Additionally, however, the absolute values of the coefficients are also used as a *penalty* to the same objective function. An additional parameter lambda ($\lambda$) controls how severe this penalty is. The following equation shows the lasso objective function for a binary response variable [adapted from @friedman_regularization_2010]:
$$
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} \underbrace{ - \biggl[ \frac{1}{N} \cdot \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \cdot \beta) - \log (1+e^{(\beta_0+x_i^T \cdot \beta)})\biggr] }_{\text{negative log-likelihood of logistic regression}} + 
 \underbrace{\biggl[ \lambda \cdot \lvert \beta \rvert \biggr]}_{ \text{regularization penalty }}
$$

::: {layout-ncol="2"}

:::: {#first-column}
**Negative log-likelihood**

- $\beta_0$: intercept
- $\beta$: vector of regression coefficients
- $p$: number of predictor variables
- $N$: number of observations
- $\frac{1}{N}$: normalise loss
- $\sum_{i=1}^N$: for each observation
- $y_i$: binary response variable for the 𝑖^th^ observation
- $x_i$: vector of predictor variables for the 𝑖^th^ observation
:::

::: {#second-column}
**Regularization penalty**

- $\lambda$: controls the strength of the penalty
- $\lvert \beta \rvert$: L1 regularisation
	- adds the sum of the absolute values of the coefficients as a penalty term
:::

::::

Ridge regression, lasso's sibling, attenuates predictions through the L2-norm. This procedure is comparable to the L1 used in lasso regression. For ridge regression, the coefficients are *squared* to serve as a penalty. Here as well, the severity of the penalty is controlled using the additional parameter lambda. We show the ridge objective function for a binary response variable [adapted from @friedman_regularization_2010]:
$$
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} \underbrace{ - \biggl[ \frac{1}{N} \cdot \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \cdot \beta) - \log (1+e^{(\beta_0+x_i^T \cdot \beta)})\biggr] }_{\text{negative log-likelihood of logistic regression}} + 
 \underbrace{\biggl[ \lambda \cdot \beta^2 \biggr]}_{ \text{regularization penalty }}
$$

::: {layout-ncol="2"}

:::: {#first-column}
**Negative log-likelihood**

- $\beta_0$: intercept
- $\beta$: vector of regression coefficients
- $p$: number of predictor variables
- $N$: number of observations
- $\frac{1}{N}$: normalise loss
- $\sum_{i=1}^N$: for each observation
- $y_i$: binary response variable for the 𝑖^th^ observation
- $x_i$: vector of predictor variables for the 𝑖^th^ observation
:::

::: {#second-column}
**Regularization penalty**

- $\lambda$: controls the strength of the penalty
- $\beta^2$: L2 regularisation
	- adds the sum of the squared values of the coefficients as a penalty term
:::

::::

While both lasso and ridge regression shrink coefficients, only lasso regression can actually bring a coefficient to zero [@van_de_velde_investigating_2019, 172]. In ridge regression, coefficients can, mathematically, only be brought *asymptotically close* to zero. Which technique is the best depends on the exact use case.

### Deciding on lambda and the split between lasso and ridge regression

Both lasso and ridge regression use the lambda parameter to determine the severity of the penalty applied during the model fitting process. This lambda value can range from $0$ (no penalty) to $+\infty$ (maximal penalty). The question then arises how one decides what value to use for lambda in order to obtain the best possible model. To compute lambda, we typically rely on *k-fold cross-validation*, a common practice in the machine learning field. We divide our dataset into *k* 'folds', effectively 'slices' of data. For example, in 10-fold cross validation, the dataset will be divided into ten parts. First, we use the nine folds to fit the model ('training set') and try a wide array of different lambda values. To examine how well each lambda value performs, we predict values using the fitted model and test how much the predictions deviate from the real values in the remaining fold ('testing set'). We then repeat this process nine more times, each time with a different fold used as a testing set. This enables us to get a global estimation of how well each possible lambda value performs, which helps combat overfitting. The lambda value which produces the best predictions across all fold combinations is chosen to fit a model on the *entire* dataset.

We saw earlier that lasso and ridge regression both have their own strengths. Elastic net regression leverages those strengths and *combines* both techniques. This means that both L1 and L2 regularisation are used at the same time, with an additional parameter alpha ($\alpha$) regulating the split between lasso and ridge penalty [adapted from @friedman_regularization_2010]:
$$
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} \underbrace{ - \biggl[ \frac{1}{N} \cdot \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \cdot \beta) - \log (1+e^{(\beta_0+x_i^T \cdot \beta)}) \biggr] }_{\text{negative log-likelihood of logistic regression}} +
 \underbrace{\lambda \cdot  \biggl[ \alpha \cdot \|\beta\|_1 + (1-\alpha) \cdot \|\beta\|_2^2/2  \biggr] }_{\text{regularization penalty}}
$$

::: {layout-ncol="2"}

:::: {#first-column}
**Negative log-likelihood**

- $\beta_0$: intercept
- $\beta$: vector of regression coefficients
- $p$: number of predictor variables
- $N$: number of observations
- $\frac{1}{N}$: normalise loss
- $\sum_{i=1}^N$: for each observation
- $y_i$: binary response variable for the 𝑖^th^ observation
- $x_i$: vector of predictor variables for the 𝑖^th^ observation
:::

::: {#second-column}
**Regularization penalty**

- $\lambda$: controls the strength of the penalty
- $\alpha$: determines the balance between L1 and L2 regularisation
- $\|\beta\|_1$: L1 norm
	- = $\|x\|_1$ with $x \in \mathbb{R}^n$ = $\|x_1\| + ... + \|x_n\|$
- $\|\beta\|_2^2/2$: adjusted L2 norm
	- = $\|x\|_2$ with $x \in \mathbb{R}^n$ = $\sqrt{x_1^2 + ... + x_n^2}$
	- division by two balances the regularization penalty
:::

::::

Alpha values range between $0$ (only ridge) and $1$ (only lasso). Values between $0$ and $1$ dictate ridge-lasso splits. In this way, ridge and lasso can be deployed optimally for the problem at hand.

## Case study: red and green word order

### Background

One area where lexical-semantic effects have long been thought to be important is the red and green word order in Dutch. This alternance is one of the best-known, most well-researched morphosyntactic alternances in Dutch. The gist of this alternance is that in verbal clusters, two verb orders are possible:

```{r, red-gloss}
#| tbl-cap: Red word order example
red_example <- as_gloss(
  "Vandaar dat hij ons opnieuw is~AUX~ aangeboden~PART~.",
  "hence that he us again was offered",
  translation = "Hence why he was offered to us again.",
  label = "red-example",
  source = "[AUX + PART]: red order (WR-P-P-G-0000250935.p.3.s.1[^gloss-source])"
)
red_example
```

```{r, green-gloss}
#| tbl-cap: Green word order example
green_example <- as_gloss(
  "Zolang ge god maar aanbeden~PART~ hebt~AUX~.",
  "as-long-as you God but worshipped have",
  translation = "As long as you've worshipped God",
  label = "green-example",
  source = "[PART + AUX]: green order (WR-P-E-A-0000223098.p.4.s.2)"
)
green_example
```

[^gloss-source]: All examples reference the SoNaR corpus [@oostdijk_d-coi_2008] part they come from.

This alternance is also possible in main clauses with three or more verbs, but it is best known in subordinate clauses. The focus in this study lies on two-word verbal clusters in subordinate clauses with the auxiliaries *hebben*, *zijn* and *worden* (perfective aspect + passive auxiliary). Some variation is also possible with modal auxiliaries, but these almost exclusively appear in the red order, which makes them much less interesting. In theory, the choice between either order is free, thus indicating that there is free variation. However, when one looks at spontaneous language use, there are some general tendencies, which we will discuss below.

#### Influence of lectal factors

Context is an important influence in the choice between the red and green word order, with a general distinction between two types of contexts. The first is the *regional* context: @de_sutter_rood_2005 [68] found that the red word order is much less preferred in Belgium than in the Netherlands, a result corroborated by @bloem_processing_2021 [206]. Still, the red order remains the dominant order in the two regions combined. The second context is register: in spoken, interactional and unedited language use, the green order is much more prevalent [@de_sutter_rood_2005, 81-87]. Inversely, this means that formal, unidirectional and edited language favours the red word order.

#### Influence of prosody

Another source of influence is that of prosody. The most important prosodical influence [@de_sutter_rood_2005, 305] stems from the so-called 'separable verbs' in Dutch. These verbs, comparable to phrasal verbs in English, have a preposition which can be detached from the root.[^separable-example] These word-initial prepositions always carry stress, giving rise to a chain of two consecutive stresses. @de_sutter_rood_2005 [154] found that speakers often choose the red order over the green order in order to avoid this chain. Consider the following example:

```{r, prosody-green-gloss}
#| tbl-cap: Green word order prosody example
prosody_green_example <- as_gloss(
  "dat ik hier mijn selectie °definitief °afgedwongen heb .",
  "that I here my selection definitively enforced have .",
  translation = "that I have definitively enforced my selection here.",
  label = "prosody_green-example",
  source = "[PART + AUX] adapted green order (WR-P-P-G-0000198602.p.7.s.4)"
)
prosody_green_example
```

If both *definitief* 'definitively' and *afgedwongen* 'enforced' are stressed, as they would typically be in Dutch, there would be two consecutive stresses. If one uses the red word order, this chain of stresses can be avoided:

```{r, prosody-red-gloss}
#| tbl-cap: Red word order prosody example
prosody_red_example <- as_gloss(
  "dat ik hier mijn selectie °definitief heb °afgedwongen .",
  "that I here my selection definitively have enforced .",
  translation = "that I have definitively enforced my selection here.",
  label = "prosody_red-example",
  source = "[AUX + PART] actual red order (WR-P-P-G-0000198602.p.7.s.4)"
)
prosody_red_example
```

[^separable-example]: e.g. *ik zoek~root~ het adres op~preposition~*, literally 'I search up the address'

#### Influence of frequency

@de_sutter_rood_2005 [289-295] found that the frequency of a participle's lemma positively correlates with the probability of that participle appearing in the red order. @bloem_processing_2021 [24] repeated the experiment and also found an effect in the same direction. @de_sutter_rood_2005 [323-324] argue that frequently appearing participles are easier to recover from memory, leaving more time for the language user to spend on (more difficult) stylistic encoding, which @de_sutter_rood_2005 then assume is the red order. 

#### Influence of priming

Another influence on the choice between red and green is priming. @de_sutter_rood_2005 [278] found that a word order is much more likely to be used if it is preceded by another cluster in the same order. A language user who has just uttered a red order is thus more included to continue using that order. Priming is considered to be the second most important influence in the choice between the two word orders [@de_sutter_rood_2005, 305].

#### Influence of semantics

Despite the fact that semantics is deemed to be so important in all linguistic frameworks (see up), semantics is the least researched area for this alternance [@bloem_processing_2021]. Here as well, the most extensive semantic research concerns macroscopic research of some semantic feature, in this case *adjectiveness*. It has been found that the more a participle is used as an adjective, the more likely this participle is to be used in the green order [@de_sutter_rood_2005, 233-242]. This is logical, since adjectives can only appear in the green order in subordinate clauses. The choice between red and green is thus 'contaminated' by the syntactic rules of the adjective [@pijpops_constructional_2018, 286-290]. Adjectiveness is thought to be the most important factor in deciding between the red and green word order [@de_sutter_rood_2005, 305], which shows that semantics is indeed an important factor to include in a morphosyntactic analysis.

### Data

To compute the semantic pull of the different verbs in the red and green word order, we collected all red and green verb clusters in subordinate clauses in the SoNaR Corpus [@oostdijk_d-coi_2008] and SoNaR New Media Corpus [@oostdijk_sonar_2014]. For more information about the data collection process and the annotation of linguistic variables, see the Appendix.

#### Overview

```{r filtered-attestations-df-load}
df_filtered <- read.csv("output/RoodGroenAnthe_sampled.csv")
```

```{r types-count}
types_count <- df_filtered$participle %>% unique %>% length
```

After filtering, `r nrow(df_filtered) %>% formatn` attestations of the red and green word order remained, belonging to `r types_count %>% formatn` unique types. `r subset(df_filtered, order == "red") %>% nrow %>% formatn` attestations belong to the red order, `r subset(df_filtered, order == "green") %>% nrow %>% formatn` to the green order. `r subset(df_filtered, country == "BE") %>% nrow %>% formatn` attestations come from Belgium, `r subset(df_filtered, country == "NL") %>% nrow %>% formatn` attestations from the Netherlands. The imbalance between Belgium and the Netherlands stems from an overall imbalance between Belgian and Netherlandic material in SoNaR. This highlights why it is so important to have a multifactorial technique which can account for these kinds of issues.

```{r auxiliary-distribution}
auxiliary_distribution <- xtabs(~ auxiliary_lemma, data=df_filtered)
auxiliary_distribution_prop <- auxiliary_distribution %>% prop.table()
```

```{r separability-distribution}
separability_distribution <- xtabs(~ separable, data=df_filtered)
separability_distribution_prop <- separability_distribution %>% prop.table()
```

```{r editing-distribution}
editing_distribution <- xtabs(~ edited, data=df_filtered)
editing_distribution_prop <- editing_distribution %>% prop.table()
```

The mean adjectiveness among all attestations is `r df_filtered$adjectiveness %>% mean() %>% formatd()` (median `r df_filtered$adjectiveness %>% median() %>% formatd()`). 
Of all attestations, `r auxiliary_distribution[["hebben"]] %>% formatn` clusters (`r auxiliary_distribution_prop[["hebben"]] %>% formatp`) feature the auxiliary *hebben* 'to have', 
`r auxiliary_distribution[["zijn"]] %>% formatn` (`r auxiliary_distribution_prop[["zijn"]] %>% formatp`) the auxiliary *zijn* 'to be' and 
`r auxiliary_distribution[["worden"]] %>% formatn` (`r auxiliary_distribution_prop[["worden"]] %>% formatp`) the auxiliary *worden* 'to become'. 
`r separability_distribution[["TRUE"]] %>% formatn` clusters (`r separability_distribution_prop[["TRUE"]] %>% formatp`) contain a separable verb. 
The mean priming rate among our attestations is `r df_filtered$priming_rate %>% mean() %>% formatd()` (median `r df_filtered$priming_rate %>% median() %>% formatd()`). 
`r editing_distribution[["TRUE"]] %>% formatn` clusters (`r editing_distribution_prop[["TRUE"]] %>% formatp`) belong to a subcorpus with assumed edited language. 
Of course, because the SoNaR corpus is a corpus of written language, there is a skew towards edited language. 
The mean type frequency of the participles in the verb clusters is `r df_filtered$FREQcount %>% mean() %>% round() %>% formatn()` (median `r df_filtered$FREQcount %>% median() %>% round() %>% formatn()`). For an overview of the proportions and distributions of the additional predictor values, see @fig-descriptive-plots.

```{r}
#| label: fig-descriptive-plots
#| fig-cap: "Overview of the proportions and distributions of the additional predictor values" 
#| column: screen-inset-shaded
#| layout-nrow: 2
#| layout-ncol: 3
#| fig-subcap:
#|   - "Distribution of adjectiveness in the dataset"
#|   - "Proportion of auxiliary lemmas in the dataset" 
#|   - "Proportion of separable verbs in the dataset" 
#|   - "Distribution of priming rate in the dataset. The rug plot shows the priming direction." 
#|   - "Proportion of edited and unedited genres in the dataset" 
#|   - "Frequency distribution of types in the dataset (log10 scale)" 
options(scipen=999)

# Adjectiveness
boxplot(df_filtered$adjectiveness, horizontal=T)

# Auxiliary verbs
barplot(auxiliary_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, auxiliary_distribution))))

# Separable verbs
row.names(separability_distribution) <- c("non-separable", "separable")
barplot(separability_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, separability_distribution))))

# Priming rate
priming_colour <- function(value) {
  if (value == 0) {
    return("black")
  }
  
  ifelse(value > 0, "red", "green")
}
priming_colour <- Vectorize(priming_colour)

boxplot(df_filtered$priming_rate, horizontal = T)
points(df_filtered$priming_rate, rep(0.5, length(df_filtered$priming_rate)), pch="|",
       col=priming_colour(df_filtered$priming_rate))

# Edited-unedited
row.names(editing_distribution) <- c("unedited", "edited")
barplot(editing_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, editing_distribution))))

# Log frequency
boxplot(df_filtered$logfreq, horizontal=T)
```

### Computing semantic pull using elastic net regression

#### Running elastic net regression

```{r meta-info-load}
source("ElasticToolsR/MetaFile.R")

model_meta <- meta.file(read.csv("output/model_meta.csv"))

alpha <- subset(model_meta$as.data.frame(), predicate == "alpha")[["object"]]
lambda <- subset(model_meta$as.data.frame(), predicate == "lambda")[["object"]]
```

The elastic net regression itself was also carried using ElasticToolsR [@sevenants_elastictoolsr_2023], an R library written for this study which acts as a wrapper around the excellent *glmnet* package for R [@friedman_regularization_2010]. We built seven predictors into our model (see @tbl-predictors); six are 'traditional' predictors, one is distributed among `r types_count %>% formatn` different coefficients: one for each type. ElasticToolsR automatically finds the appropriate alpha and lambda values which deliver the best possible model. Our model for the red and green order performs the best at alpha = `r alpha %>% formatd(2)` and lambda = `r lambda %>% formatd(6)`.

| predictor | type | possible values |
| --- | --- | --- |
| participle | distributed binary values for each type | in verb cluster/not in verb cluster |
| country | binary | BE/NL | 
| separable | binary | yes/no |
| edited | binary | yes/no |
| adjectiveness | real number | $0$ to $1$ |
| priming_rate | real number | $-\infty$ to $+\infty$ |
| logfreq | natural number | $0$ to $+\infty$ |


: An overview of the predictors for the elastic net regression {#tbl-predictors}

#### Results

```{r coefficients-data-frame}
coefficients <- read.csv("output/RoodGroenAnthe_coefficients.csv")
coefficients_count <- dim(coefficients)[[1]]

coefficients$removed <- ifelse(coefficients$coefficient == 0, TRUE, FALSE)

removed_distribution <- xtabs(~ removed, data=coefficients)
removed_distribution_prop <- removed_distribution %>% prop.table()

coefficients_non_zero <- coefficients[coefficients$removed == FALSE,]
coefficients_non_zero$order <- ifelse(coefficients_non_zero$coefficient > 0, "red", "green")
non_zero_distribution <- xtabs(~ order, data=coefficients_non_zero)
non_zero_distribution_prop <- non_zero_distribution %>% prop.table()
```

```{r chi-square-test-load}
source("3-1 Chi-square.R")
```

Out of the `r coefficients_count %>% formatn` coefficients in our elastic net model, 
`r removed_distribution[["TRUE"]] %>% formatn` (`r removed_distribution_prop[["TRUE"]] %>% formatp`) 
coefficients were removed and 
`r removed_distribution[["FALSE"]] %>% formatn` (`r removed_distribution_prop[["FALSE"]] %>% formatp`)
coefficients were retained (@fig-coefficient-plots-1).
All removed coefficients belong to the `Participle` predictor, which means all our 'traditional' predictors were retained by the model.

##### Lexical effects

We will first, however, look at our `Participle` predictor. The coefficients which correspond to the different participles in the verb cluster attestations give us an idea of whether those participles tend to the red or green order. Since our model has the red word order as response variable $1$, positive coefficients show logit corrections towards the red word order, while negative coefficients show logit corrections towards the green word order. We can interpret these attractions as semantic pulls.
Of the retained coefficients, 
`r non_zero_distribution[["green"]] %>% formatn` (`r non_zero_distribution_prop[["green"]] %>% formatp`) 
coefficients represent participles that tend to the green word order, 
`r non_zero_distribution[["red"]] %>% formatn` (`r non_zero_distribution_prop[["red"]] %>% formatp`) coefficients represent participles that tend to the red word order (see @fig-coefficient-plots-2). If we perform a $\chi^2$ test on this result, we retrieve a $\chi^2$ value of `r chi_square_test$statistic %>% formatd` with a p value of `r chi_square_test$p.value %>% formatd(3)`, which indicates that the proportion of the two orders is significantly different from a regular 50-50 split.

From the distribution of the coefficients of both orders (see @fig-coefficient-plots-3), we can also infer that green word order associations are generally stronger, and also feature more extreme logit corrections. The extreme green corrections are no doubt related to the strong effect of adjectiveness (see below). Our result is completely in line with the results from both @de_sutter_rood_2005 [247-248] and @bloem_processing_2021 [138]. While @de_sutter_rood_2005 claim that the green order is more lexically specific, @bloem_processing_2021 counters this by claiming that it is simply more difficult for red orders to form lexical associations, since the red order is by default more frequent:

> The [red] order is more frequent, therefore the threshold is higher for verbs to be significantly
associated with the [red] order -- it requires a higher frequency. Similarly, the
threshold for being significantly associated with the less frequent [green] order is
lower, and more verbs are significantly associated with it. [@bloem_processing_2021, 138]

```{r}
#| label: fig-coefficient-plots
#| column: screen-inset-shaded
#| layout-nrow: 2
#| layout-ncol: 3 
#| fig-subcap:
#|   - "Proportion of removed and retained coefficients"
#|   - "Proportion of preference for either the red or green word" 
#|   - "Distribution of participle coefficients" 
options(scipen=999)

# Retained-removed distribution
row.names(removed_distribution) <- c("retained", "removed")
barplot(removed_distribution %>% sort(decreasing=TRUE),
        ylim=range(pretty(c(0, removed_distribution))))

# Red-green distribution
barplot(non_zero_distribution %>% sort(decreasing=TRUE),
        col=c("green","red"),
        ylim=range(pretty(c(0, non_zero_distribution))))

# Value distribution
boxplot(coefficient ~ order, col=c("green","red"), pch=19, 
        horizontal=T, data=coefficients_non_zero)
```

##### Other effects and intercept

Recall that we added six additional predictors under multifactorial control in order to make sure that we did not lump together variation from other sources into our lexical effects. As was mentioned before, all these additional predictors were retained by the elastic net regression, which means that they are useful to model the choice between the red and green word order. This is, of course, unsurprising, since we based ourselves on previous research for the selection of these predictors. @tbl-other-coefficients shows an overview of the additional predictors and their coefficient values. Note that though elastic net regression does not provide significance levels, whether a predictor is retained also reflects the importance of that predictor in the model.

```{r #tbl-other-coefficients}
#| label: tbl-other-coefficients
#| tbl-cap: "Overview of the additional predictors, their values and whether they were retained by the elastic net regression" 

# All other coefficients start with an underscore
other_coefficients <- coefficients[coefficients$feature %>% substr(1,1) == "_",]
# Change "removed" to "retained"
other_coefficients$retained = ifelse(other_coefficients$removed, "no", "yes")
# Friendly names
other_coefficients$feature <- gsub("_(.+)", "\\1", other_coefficients$feature)
other_coefficients$feature <- gsub("(is|_)", " ", other_coefficients$feature)
other_coefficients$predictor <- other_coefficients$feature
# Only keep interesting columns
other_coefficients <- other_coefficients[, names(other_coefficients) %in% c("coefficient", "predictor", "retained")]
# Re-order
other_coefficients <- other_coefficients[,c(3, 1, 2)]
# Sort
other_coefficients <- other_coefficients[order(other_coefficients$coefficient,
                                               decreasing=TRUE),]
# Remove row indices
rownames(other_coefficients) <- NULL
# Output to document
other_coefficients
```

```{r intercept-computation}
intercept <- subset(model_meta$as.data.frame(), predicate == "intercept")[["object"]]
```

Before we discuss the values of the additional predictors, we look at the intercept of our model and the situation it represents. The intercept for our model has a logit value of `r intercept %>% formatd()`, which means that the 'baseline' syntactic realisation is the green order. This might seem strange -- the red order is said to be the most frequent -- but our baseline situation represents a situation for (1) a non-separable verb (2) from the perspective of a Belgian speaker (3) in an unedited genre (4) without priming (5) for a verb with a log frequency of zero (6) with an adjectiveness of zero. The corrections stemming from the aditional predictors shift the logit in the directions we know from the literature. If the verb is separable, the logit increases towards the red order. The same applies for when we shift the perspective to a Netherlandic speaker, when the genre is edited and when priming is at play. We also see that as the log frequency increases, so does the logit correction towards the red order, exactly as expected. In the other direction, we see that maximal adjectiveness ($\text{adjectiveness} = 1$) dramatically shifts the red-green choice towards the green order. This is also indeed what we expect.

### Attaching insights into meaning with distributional semantics

While we now have a window to the semantic preferences of participles in the red and green word order, we only see the *surface* of our semantic effects. We do not see the general semantic motivations behind the choice for either word order. To reach closer to this general semantic understanding, we made use of semantic vectors. Semantic vectors are mathematical vector representations which try to capture the meaning of words [@montes_cloudspotting_2021]. The representations hinge on statistical co-occurrences -- similar words appearing in similar contexts. An important property of semantic vectors for this study is that semantically related or comparable words tend to have similar vectors. This means that, if we can somehow inspect the vector space, we might be able to find semantic 'pockets' -- areas of words with similar meanings -- which are conducive to either word order.

As a basis for our semantic analysis, we used the 'count' snaut vectors for Dutch [@mandera_explaining_2017] -- a ready-made package with semantic vectors for almost any Dutch word imaginable. While these vectors are constructed on the *type* level, which means that homonyms receive a single vector, we did not expect a high level of homonymy for participles and thus did not expect issues for our study.

One of the other properties of semantic vectors is that they are high-dimensional. For example, a snaut vector has a length of 200, which means that a single word is represented by 200 numbers. If we want to inspect the vector space of our words visually, we have to reduce the vectors to two or three dimensions. Typically, this is done by first computing a distance matrix, i.e. a matrix which shows the pairwise distances between each word, and then applying dimension reduction to that matrix. The dimension reduction technique will attempt to capture the information found in the high-dimensional space as well as possible in the lower-dimensional space. The added benefit is that much of the noise present in the vectors will be eliminated in this reduction process. While there are several dimension reduction techniques available, we chose MDS [@douglas_carroll_multidimensional_1998], TSNE [@hinton_stochastic_2002] and UMAP [@mcinnes_umap_2020], following @montes_cloudspotting_2021.

The dimension reduction in this study is carried out using *DistributionalSemanticsR* [@sevenants_distributionalsemanticsr_2023], another library made for this study. The library is a wrapper around *vegan* [@oksanen_vegan_2022], *Rtsne* [@krijthe_rtsne_2015] and *umap* [@konopka_umap_2022] and facilitates the dimension reduction process. We reduced the dimensions to 2 in order to be able to inspect the distance between the vectors visually. For a visual representation of the semantic field processed by the three dimension reduction techniques, see @fig-mds-test.

```{r visualisation-load}
#| output: false
source("2-1 Visualisation basic.R")
```

```{r #fig-mds-test}
#| fig-cap: Visual representations of the semantic field in MDS, TSNE and UMAP
#| column: screen-inset-shaded
#| layout-ncol: 3 
#| fig-width: 5
#| fig-height: 5
#| fig-subcap:
#|   - "Visual representation of the semantic field, MDS dimension reduction"
#|   - "Visual representation of the semantic field, TSNE dimension reduction"
#|   - "Visual representation of the semantic field, UMAP dimension reduction"

plot_basic_coords("mds", TRUE)

plot_basic_coords("tsne", TRUE)

plot_basic_coords("umap", TRUE)
```

The three constellations of distributional vectors do not immediately show a clear-cut emergent structure which we can relate to our red and green word order. At the same time, any 'manual' grouping we would make would be undesirable anyway, since it would likely be biased by human perception. Therefore, we instead focussed on clustering and spatial modelling of the semantic coordinates. In the next section, we will discuss two ways we attempted to group the data and how we tried to avoid choosing a specific clustering ourselves.

#### Clustering

First, we attempted to use regular clustering techniques in an attempt to bring structure to the data. We clustered the semantic data points using both PAM and Dbscan.

##### PAM

The PAM (Partitioning Around Medoids) technique [@kaufman_partitioning_1990] selects a predefined number of data points as cluster centres and then builds clusters around those central points. We used the R implementation in the cluster package [@maechler_cluster_2022]. Because a predefined number of clusters is required and we have no prior knowledge on the desirable number of clusters, we built clusterings with a number of clusters ranging from 2 to 60. The upper limit is a practical choice made to avoid compute issues at larger cluster numbers.

Because semantics is inherently messy, we made the choice to remove all participles which did not fit their cluster very well from their respective clusters. For this, we used the silhouette width metric [@rousseeuw_silhouettes_1987] in its R implementation for semantics [@speelman_semvar_2014]. All participles with a negative silhouette width -- generally speaking, participles closer to participles from other clusters than to those of their own -- were assigned a zero cluster.

To investigate the best clustering among the 59 clusterings we made, we built a linear regression model for each clustering which attempted to model the coefficients from the Elastic Net model, following @levshina_radically_2014:
$$
coefficient \sim cluster
$$
A 'good' clustering for our purpose would be a clustering on the basis of which one can accurately predict the relative preference for the red or green word order. To measure the predictive performance of clusters, the $R^2$ measure is used [implementation in R by @barton_mumin_2023]. @fig-pam-r2 gives an overview of the $R^2$ values for each model of the different dimension reduction techniques by $k$ value:

```{r pam-load}
#| output: false
source("2-2 PAM vis.R")
```

```{r #fig-pam-r2}
#| fig-cap: Overview of the R^2^ values for each $k$-model for all three dimension reduction techniques
#| column: screen-inset-shaded
#| layout-ncol: 3
#| fig-subcap:
#|   - "R^2^ value of each model by $k$ value, MDS"
#|   - "R^2^ value of each model by $k$ value, TSNE" 
#|   - "R^2^ value of each model by $k$ value, UMAP"

plot_bar(mds_results_pam, "r2")

plot_bar(tsne_results_pam, "r2")

plot_bar(umap_results_pam, "r2")
```

It is unclear from the plots why certain $k$ values yields the better results. In the TSNE plot, we see that many of the best models are those which feature many clusters at once. This is not surprising, since having many clusters is a form of overfitting. With a high number of clusters, one can more easily create smaller, homogeneous clusters and therefore obtain better predictions. In the other plots, the explanations are not as clear-cut. We see for MDS that using three clusters perform the best, while 22 clusters yield the best result for UMAP.

In order to get a better understanding of how well our regression models capture the patterns in the data, we assigned random clusterings to the data points and built 'garbage' regression models based on these assigned clusters. We then used Vuong's test [@vuong_likelihood_1989; implementation in R by @merkle_nonnest2_2020] to check whether there was a statistically significant difference between the models based on the real clusters and the models based on the randomly assigned clusters. If a difference was found, we checked whether the real model performed significantly better than the garbage model (or vice versa).

@fig-pam-vuong-p shows the significance values for the difference between the real and garbage models. As can be inferred from the graphs, not a single model was substantially different from the garbage model. This means that the clustering using PAM essentially failed, since one could just as well use randomised clusterings and obtain comparable results.

```{r #fig-pam-vuong-p}
#| fig-cap: Overview of the Vuong test p values for each comparison between a $k$-model and its garbage counterpart for all three dimension reduction techniques
#| column: screen-inset-shaded
#| layout-ncol: 3
#| fig-subcap:
#|   - "p values of the comparisons of each model with its garbage counterpart by $k$ value, MDS"
#|   - "p values of the comparisons of each model with its garbage counterpart by $k$ value, TSNE" 
#|   - "p values of the comparisons of each model with its garbage counterpart by $k$ value, UMAP"

plot_bar(mds_results_pam, "vuong_p")

plot_bar(tsne_results_pam, "vuong_p")

plot_bar(umap_results_pam, "vuong_p")
```

##### Dbscan

```{r dbscan-load}
#| output: false
source("2-3 Dbscan vis.R")
dbscan_model_count <- dim(mds_results_dbscan)[1]
```

The Dbscan algorithm [@ram_density_2010] clusters data points which are closely packed together. Because the algorithm can by itself decide whether data points do not fit well into any cluster, it might be a better choice for our messy semantic data. We used the R implementation in the dbscan package [@hahsler_dbscan_2019].

Dbscan hinges on two parameters to come to a clustering solution: one parameter (`eps`) decides how close two points need to be in order to be considered 'dense' enough to form a cluster, the other (`pts`) decides how many neighbours a point needs to have in order to be considered a salient point in the points cloud. Again, we considered a wide range of possible parameter combinations[^dbscan-combi], which yielded `r dbscan_model_count %>% formatn` models for each dimension reduction technique. Note that the number of clusters is variable in dbscan and depends on the set `eps` and `pts` values. @fig-dbscan-cluster-count shows an overview of the number of clusters in each of the models. For clarity's sake, a graph with log-transformed counts is also given.

```{r #fig-dbscan-cluster-count}
#| fig-cap: Overview of the number of clusters in each dbscan clustering for all three dimension reduction techniques
#| column: screen-inset-shaded
#| layout-ncol: 3
#| layout-nrow: 2
#| fig-subcap:
#|   - "Number of clusters by `pts` and `ets` value, MDS"
#|   - "Number of clusters by `pts` and `ets` value, TSNE" 
#|   - "Number of clusters by `pts` and `ets` value, UMAP"
#|   - "Number of clusters by `pts` and `ets` value, MDS (log scale)"
#|   - "Number of clusters by `pts` and `ets` value, TSNE (log scale)" 
#|   - "Number of clusters by `pts` and `ets` value, UMAP (log scale)"

plot_tile(mds_results_dbscan, "cluster_count")
plot_tile(tsne_results_dbscan, "cluster_count")
plot_tile(umap_results_dbscan, "cluster_count")

plot_tile(mds_results_dbscan, "cluster_count_log")
plot_tile(tsne_results_dbscan, "cluster_count_log")
plot_tile(umap_results_dbscan, "cluster_count_log")
```

[^dbscan-combi]: All combinations from $\text{pts}=2$ to $\text{pts}=100$ (increments of $1$) and $\text{eps}=0$ to $\text{eps}=1$ (increments of $0.01$).

To investigate the best clustering among the `r dbscan_model_count %>% formatn` clusterings we made, we built a regression for each clustering, which again attempted to model the coefficients from the Elastic Net model:

$$
coefficient \sim cluster
$$

Again, we used the $R^2$ values of the models to find the best models and compared the models to garbage models with randomly assigned clusters using the Vuong test. Here as well, the significance values show that no discernible difference can be found between real and garbage models, which means that for dbscan as well, clustering has failed. @fig-dbscan-r2-vuong-p shows both the R^2^ values and Vuong test p values.

```{r #fig-dbscan-r2-vuong-p}
#| fig-cap: Overview of the R^2^ values for each $k$-model and Vuong test p values for each comparison between a $k$-model and its garbage counterpart for all three dimension reduction techniques
#| column: screen-inset-shaded
#| layout-ncol: 3
#| layout-nrow: 2
#| fig-subcap:
#|   - "R^2^ value of each model by $k$ value, MDS"
#|   - "R^2^ value of each model by $k$ value, TSNE" 
#|   - "R^2^ value of each model by $k$ value, UMAP"
#|   - "p values of the comparisons of each model with its garbage counterpart by $k$ value, MDS"
#|   - "p values of the comparisons of each model with its garbage counterpart by $k$ value, TSNE" 
#|   - "p values of the comparisons of each model with its garbage counterpart by $k$ value, UMAP"

plot_tile(mds_results_dbscan, "r2")
plot_tile(tsne_results_dbscan, "r2")
plot_tile(umap_results_dbscan, "r2")

plot_tile(mds_results_dbscan, "vuong_p")
plot_tile(tsne_results_dbscan, "vuong_p")
plot_tile(umap_results_dbscan, "vuong_p")
```

#### Generalised Additive Models

It is clear that it is a difficult task to form meaningful clusters from our semantic data. Therefore, we instead opted to focus on the global semantic space and model the elastic net coefficients directly using the coordinates of the participles in the dimensionally-reduced semantic space. In short, this means we tried to predict the coefficient of a data point (towards red/green) on the basis of its position in the semantic space. This is, at least theoretically, possible using a simple linear regression:

$$
coefficient \sim x + y
$$

@tbl-lm gives summaries of the regression models for the MDS, TSNE and UMAP coordinates. Note that the data points eliminated in the elastic net regression are not part of the model.

```{r gam-load}
#| output: false
source("2-4 GAM.R")
library(broom)
library(dplyr)

format_num_col <- function(p_value) {
  if (p_value < 0.01) {
    return("<0.01")
  } else {s
    return(formatd(p_value, 4))
  }
}
format_num_col <- Vectorize(format_num_col)

format_table <- function(df) {
  df <- df %>% tidy

  #df$sig <- ifelse(df$p.value <= 0.05, "*", "")
  
  df$estimate <- df$estimate %>% formatd(4)
  df$std.error <- df$std.error %>% formatd(4)
  df$statistic <- df$statistic %>% formatd(4)
  df$p.value <- df$p.value %>% format_num_col()

  return(df)
}

```

```{r #tbl-lm}
#| tbl-cap: Summaries of the regression models predicting the elastic net coefficients based on the semantic space
#| column: screen-inset-shaded
#| layout-ncol: 1
#| tbl-subcap:
#|   - "Summary of the regression model predicting the elastic net coefficients based on the semantic space, MDS"
#|   - "Summary of the regression model predicting the elastic net coefficients based on the semantic space, TSNE"
#|   - "Summary of the regression model predicting the elastic net coefficients based on the semantic space, UMAP"

lm_mds <- build_lm(df, "mds", "non_zero")
lm_mds %>% format_table()

lm_tsne <- build_lm(df, "tsne", "non_zero")
lm_tsne %>% format_table()

lm_umap <- build_lm(df, "umap", "non_zero")
lm_umap %>% format_table()
```

We see that the results differ for each regression model. For MDS, the further a participle is situated along the x and y-axis, the more likely it is to appear in the green order. For TSNE, the further a participle is situated along the y-axis, the more likely it is to appear in the red order. UMAP does not show any significant semantic trends. The MDS and TSNE plots show that at least *some* semantic effect is at play here. If we *plot* the predictions of the models, however (@fig-lm), it becomes immediately clear that using two linear predictors for spatial data -- in this case *virtual* spatial data -- is not ideal[^gaps]:

[^gaps]: Extrapolations are only shown 0.1 units away from the furthest data point.

```{r #fig-lm}
#| fig-cap: Visualisations of the regression models predicting the elastic net coefficients based on the semantic space
#| column: screen-inset-shaded
#| layout-ncol: 3
#| fig-width: 6
#| fig-height: 5
#| fig-subcap:
#|   - "Visualisation of the regression model predicting the elastic net coefficients based on the semantic space, MDS (non-eliminated types only)"
#|   - "Visualisation of the regression model predicting the elastic net coefficients based on the semantic space, TSNE (non-eliminated types only)"
#|   - "Visualisation of the regression model predicting the elastic net coefficients based on the semantic space, UMAP (non-eliminated types only)"

plot_gam(df, lm_mds, "mds", "non_zero", 0.1)

plot_gam(df, lm_tsne, "tsne", "non_zero", 0.1)

plot_gam(df, lm_umap, "umap", "non_zero", 0.1)
```

These linear models are far too simple to accurately represent the semantic space. This is of course inherent to the design of such models, since they can only model the two axes linearly.

Due to the limitations of linear regression, we opted to use a modern regression technique to model the spatial coordinates: Generalised Additive Modelling (GAM) [@hastie_generalized_1987]. GAM models are able to model non-linear relationships and can be used to model spatial relationships when two axes are deployed as a joint predictor. GAM models have already been used in linguistic research to model linguistic phenomena spatially [@wieling_quantitative_2011; @franco_maps_2019; @ghyselen_over_2019]. We can use GAMs for the same purpose to model the spatial distribution of the red and green word order in our virtual semantic space according to the following equation:

$$
coefficient \sim te(x + y)
$$

We used a *tensor smooth* (represented as $te$ in the equation) to model the x and y axis jointly.

We show the same heatmap as before, but this time using the underlying GAM models (@fig-gam).

```{r gam-gam}
#| output: false

gam_mds <- build_gam(df, "mds", "non_zero")
gam_tsne <- build_gam(df, "tsne", "non_zero")
gam_umap <- build_gam(df, "umap", "non_zero")
```

```{r #fig-gam}
#| fig-cap: Visualisations of the GAM models predicting the elastic net coefficients based on the semantic space
#| column: screen-inset-shaded
#| layout-ncol: 3
#| fig-width: 6
#| fig-height: 5
#| fig-subcap:
#|   - "Visualisation of the GAM model predicting the elastic net coefficients based on the semantic space, MDS (non-eliminated types only)"
#|   - "Visualisation of the GAM model predicting the elastic net coefficients based on the semantic space, TSNE (non-eliminated types only)"
#|   - "Visualisation of the GAM model predicting the elastic net coefficients based on the semantic space, UMAP (non-eliminated types only)"

plot_gam(df, gam_mds, "mds", "non_zero", 0.1)

plot_gam(df, gam_tsne, "tsne", "non_zero", 0.1)

plot_gam(df, gam_umap, "umap", "non_zero", 0.1)
```

It is immediately clear that the GAM models are much better at capturing patterns in the semantic space. We also see general semantic areas appearing, where the model predicts data points to generally be more red or more green. Still, if we look at the actual data points, the semantic field remains quite messy. Nonetheless, it seems that the GAM models are much better at dealing with semantic noise than the clustering algorithms.

With the affirmation of the existence of these red and green lexical areas, the question arises *what* verbs we find in these areas. What semantic properties make a verb particularly red or green? To investigate the contents of the different semantic areas, we attempted to find an explanation in two ways: the overall adjectiveness value in different semantic areas, and their respective densities.

To investigate both aspects, we first subdivided the semantic space into nine square subareas. In the top-left corner of each square, we show the number of data points in that square (first line) and the mean adjectiveness of the points in that square (second line). Each square is assigned a so-called 'dominant colour'. This dominant colour is based on a t-test of the coefficients of the data points in that square. The null hypothesis for each square is that the coefficients do not specifically pull towards either word order. If significance was not attained in the t-test, the square receives a grey outline to indicate the lack of a dominant colour. If significance was attained in the t-test for that square, we assumed a dominant colour on the basis of the t-test estimate's sign; a negative estimate shows a square's pull towards the green order, a positive estimate shows a pull towards the red order. @fig-gam-squares shows an overview of the squares and their dominant colours.

```{r #fig-gam-squares}
#| fig-cap: Visualisations of the GAM models predicting the elastic net coefficients based on the semantic space, with subdivision squares and significantly differing squares indicated
#| column: screen-inset-shaded
#| layout-ncol: 3
#| fig-width: 6
#| fig-height: 5
#| fig-subcap:
#|   - "Visualisation of the GAM model predicting the elastic net coefficients based on the semantic space, with subdivision squares and significantly differing squares indicated, MDS (non-eliminated types only). Each line connects areas with significantly differing adjectiveness values."
#|   - "Visualisation of the GAM model predicting the elastic net coefficients based on the semantic space, with subdivision squares and significantly differing squares indicated, TSNE (non-eliminated types only). Each line connects areas with significantly differing adjectiveness values."
#|   - "Visualisation of the GAM model predicting the elastic net coefficients based on the semantic space, with subdivision squares and significantly differing squares indicated, UMAP (non-eliminated types only). Each line connects areas with significantly differing adjectiveness values."

source("2-4 GAM squares.R")

squares_test(df, "mds", "non_zero", -1.8, 1.7, 3.2, 3, 0.1)

squares_test(df, "tsne", "non_zero", -5, 5, 10, 3, 0.1)

squares_test(df, "umap", "non_zero", -2.8, 2.6, 5.25, 3, 0.1)

```

Interestingly, the plot of each dimension reduction technique only shows four squares with a dominant colour, which is always green. This corroborates the idea by @de_sutter_rood_2005 that the green word order is more lexically specific, since it clearly captures a larger proportion of the semantic space. The red order is more evenly dispersed over the whole plane. To find an explanation for the presence or absence of a dominant colour of a square, we turned to adjectiveness, as it is explained to be the most important external factor in deciding between red and green [@de_sutter_rood_2005, 305]. We conducted pairwise t-tests of the adjectiveness values of the verbs in each square: this means that we conducted t-tests between each combination of squares. @fig-gam-squares indicates which squares have significantly differing adjectiveness values through the means of a line connecting significantly different pairs. There does not seem to be any sort of pattern where all dominantly green squares differ significantly from squares with a non-dominant colour. On the contrary, we even find significant differences in adjectivity *among* green squares. This indicates that the multifactorial control in the regression analysis, where adjectiveness is included as a fixed effect, worked as intended, and that there is no lingering 'effect' of this variable in the semantic space.

Next, we turned to the density of each square, i.e. the number of data points found in each square. We built a regression model for each dimension reduction technique:

$$
\text{has dominant order?} \sim \text{\#data points}
$$

```{r #tbl-data-point-glm}
#| tbl-cap: Summaries of the regression models predicting the presence of a dominant colour
#| column: screen-inset-shaded
#| layout-ncol: 1
#| tbl-subcap:
#|   - "Summary of the regression model predicting the presence of a dominant colour, MDS"
#|   - "Summary of the regression model predicting the presence of a dominant colour, TSNE"
#|   - "Summary of the regression model predicting the presence of a dominant colour, UMAP"

glm_mds <- do_squares_regression(df, fit, "mds", "non_zero", -1.8, 1.7, 3.2, 3, 0.1)
glm_mds %>% format_table()

glm_tsne <- do_squares_regression(df, fit, "tsne", "non_zero", -5, 5, 10, 3, 0.1)
glm_tsne %>% format_table()

glm_umap <- do_squares_regression(df, fit, "umap", "non_zero", -2.8, 2.6, 5.25, 3, 0.1)
glm_umap %>% format_table()
```

Only the MDS logistic regression show a significant effect of density. For the other dimension reduction techniques, the regression models fail to reach significance. It is therefore gratuitous to attach any firm conclusions to the relationship between density and a dominant colour.

The question is, then: what possible explanation remains to shed light on the patterns found in the semantic space? Interestingly, the difficulty in finding a specific pattern might be the explanation itself. Indeed, the lexical specificity of a language can function as an aspect that is hard to master, and therefore has value as a social marker. As @hurford_origins_2014 [49] states, "the complexity of languages is a signal of group membership". In their computer simulation on human cooperation, @nettle_social_1997 have shown that it is beneficial to devise an adequately complex "distinctive code" [-@nettle_social_1997 98] in order to provide stability in large social groups. The lexical patterns we find here could be an example of such a social marker and distinctive code. In this light, the red and green word order are quite interesting. While the alternance serves no meaning difference, and variation is, in principle, 'free', the many lexical attractions found for different verb types in Dutch allow language users to pick up on subtle differences, which they can use to decide on group membership. We leave this evolutionary speculation to further research.

## Conclusion {#sec-conclusion}

In this study, we investigated possible lexical effects of the red and green word order in Dutch subordinate clauses. To this end, we used elastic net regression, an innovative regression technique which promotes lexical effects to first-class predictors *and* is robust against data-related issues induced by the typical Zipfian distribution of the lexicon. The methodology withstood the proof-of-concept tests and yielded estimates of semantic pulls for all retained verbs in the regression. The elastic net methodology is particularly useful for lexical research, where a lexical influence predictor (like 'verb type' in this case) can have potentially hundreds of different levels. Elastic net regression can disable certain levels of this predictor if necessary to boost the quality of the model, which makes it robust against the aforementioned Zipfian-related troubles and prevents overfitting. As such, the methodology seems promising for further research into other lexical effects and other alternances.

In addition, the extensive prior research on the red and green word order in Dutch [e.g. @de_sutter_rood_2005; @bloem_processing_2021] proved to be a great help for this study. The elastic net technique allowed us -- through its multifactorial control -- to further build upon the established knowledge and focus on the lexical effects proper. All intra- and extralinguistic variables were retained by the regression, and their effects pointed in the same directions as were found in prior research.

Next, we saw that the Generalised Additive Models (GAMs) were proved to be a useful technique for modelling the messy semantic spaces built from distributional semantic data. Their ability to fit complex curves and therefore model complex phenomena can be exploited to create two-dimensional heatmaps which can show us where in the semantic space we find specific pulls towards either choice in a typical morphosyntactic alternance. In our case, the heatmaps showed locations in the semantic space where verbs are specifically attracted towards the red or the green word order, or neither of the two. While the semantic pulls produced by the elastic net regression already proved that lexical effects indeed exist for this alternance, the GAM plots show that there is also at least *some* semantic structure to them. This structure, however, cannot easily be related to more general semantic features, and it is possible that its opacity is functional in the sense that it is part of the idiomaticity of the Dutch language, and as such, functions as a social marker.

Future research could zoom in on the contribution of phonological similarity between words to see whether this also contributes to the lexical effects we see. Of course, there are many other possible alternances that could be investigated. Examples are the distinction between Dutch *doen* and *laten* ['make' and 'let', e.g. @speelman_causes_2009] or the dative alternance in English [@bresnan_predicting_2007]. In any case, this study has shown that combining elastic net regression and generalised additive models can bring us one step further in research centred around lexical effects.

## Appendix {#appendix .appendix}

### Try it yourself

- Misschien ook een aansporing om zelf naar de coëfficiënten te kijken met Rekker — met automatische link

### Corpus and querying

To compute the semantic pull of the different verbs in the red and green word order, we collected all red and green verb clusters in subordinate clauses in the SoNaR Corpus [@oostdijk_d-coi_2008] and SoNaR New Media Corpus [@oostdijk_sonar_2014]. Since we are interested in syntactic alternances, we needed a syntactically informed corpus format ('treebank') in order to reliably find the attestations we need. While the SoNaR corpus does not ship with syntactic information, much of the corpus material from SoNaR is also included in the Lassy corpus [@van_noord_large_2013], which *is* syntactically annotated using Alpino [@van_noord_at_2006]. We retrieved the syntactic information from SoNaR available in Lassy, and parsed the remaining sentences using Alpino ourselves. This left us with a fully syntactically informed SoNaR corpus, ready to be queried for red and green word orders.

```{r}
df_original <- read.csv("RoodGroenAnthe.csv")
```

In order to query the syntactic information of the entire SoNaR corpus, we used mattenklopper [@sevenants_mattenklopper_2023], a treebank search engine tailor-made for this study. While there are several Alpino search engines available, many of which are much more user friendly *and* faster than the custom search engine used here [e.g. GrETEL, @augustinus_example-based_2012; PaQu, @kleiweg_paqu_2023], these engines all have specific problems which made it so they could not be used for this study. In both GrETEL and PaQu, it is only possible to retrieve entire sentences. One cannot further retrieve the participles or auxiliaries in a verb cluster -- this must be done manually. GrETEL only supports searching through subsections of SoNaR[^gretel] and PaQu does not even offer the SoNaR corpus for querying. Finally, GrETEL results are limited to only 500 sentences due to copyright concerns, which is not enough for a sophisticated analysis. The custom mattenklopper engine was developed as a solution to all these problems. It is available online and can be used for future alternance studies of Dutch using Alpino-based corpora. The xpath queries used to search the corpus are included in the appendix. The mattenklopper search engine returned `r nrow(df_original) %>% formatn` attestations of either the red or green word order.

[^gretel]: In addition, the SoNaR corpus included in the GrETEL web interface is parsed using an older version of Alpino, which results in unreliable annotations.


### Filtering and enriching

```{r}
filtering_numbers <- read.csv("output/filtering_numbers.csv")
```

The results were further filtered in order to guarantee the quality of the attestations. In short, duplicates were removed, tokenisation errors were fixed (i.e. removing superfluous punctuation from participles) and obvious tagging mistakes were removed (e.g. words such as *zgn* 'so-called' and *gemiddeld* 'average' were removed). In addition, wrong participle endings (e.g. *gebeurt* instead of *gebeurd* 'happened') were corrected using *naive-dt-fix* [@sevenants_naive-dt-fix_2023], a library for the R language designed for this study. This library automatically corrects wrong participle endings by relying on the relative frequencies of all possible spellings. The most frequent spelling is seen as the correct spelling and is used as a correction.[^dt] Declensed words were also removed (e.g. *geplaatst**e***). Past participles cannot be declensed in Dutch, so all declensed forms in the corpus tagged as participles are, in fact, mis-tagged adjectives. We also removed all clusters with an auxiliary other than *hebben*, *zijn* or *worden* and removed all attestations without a sentence ID (which we need to compute priming). In addition, all types occurring less than 10 times were removed in order to guarantee a stable estimate for the semantic pulls of each type. As a result of these operations, `r subset(filtering_numbers, name == "first_removal")[1,][["no_items"]] %>% formatn` attestations were removed.

[^dt]: Some participles are so often misspelled that they are seen as the correct spelling by the program. These cases are overruled by a built-in list of 'problematic' participles which have their correct spellings enforced.

Furthermore, the attestations were enriched with additional information to be used in the multifactorial elastic net regression. Firstly, regional information was added for each attestation. SoNaR comes with contextual information about its documents, such as country of origin information. Since region is an important influence in the red-green word order, this variable is vital for multifactorial control.

We also used the subcorpus division in SoNaR (e.g. `WR-P-E-A_discussion_lists`, `WR-P-E-F_press_releases`) to distinguish between edited and unedited genres. We decided to focus on an edited-unedited dichotomy, because it is difficult to assess the formality of certain genres in the corpus (e.g. websites and blogs). By focussing on whether a genre is typically edited or not, we sidestep these issues, but we are still able to include some form of formality distinction. Refer to @tbl-sonar-editing for an overview of our judgements.

| subcorpus | contents | degree of editing |
|---|---|---|
| WR-P-E-A | discussion lists | unedited |
| WR-P-E-C | e-magazines | edited |
| WR-P-E-E | newsletters | no attestations[^no-attestations] |
| WR-P-E-F | press releases | edited |
| WR-P-E-G | subtitles | edited |
| WR-P-E-H | teletext pages | edited |
| WR-P-E-I | web sites | edited |
| WR-P-E-J | wikipedia | edited |
| WR-P-E-K | blogs | edited |
| WR-P-P-B | books | edited |
| WR-P-P-C | brochures | edited |
| WR-P-P-D | newsletters | edited |
| WR-P-P-E | guides manuals | edited |
| WR-P-P-F | legal texts | edited |
| WR-P-P-G | newspapers | edited |
| WR-P-P-H | periodicals magazines | edited |
| WR-P-P-I | policy documents | edited |
| WR-P-P-J | proceedings | edited |
| WR-P-P-K | reports | edited |
| WR-U-E-E | written assignments | edited |
| WS-U-E-A | auto cues | edited |
| WS-U-T-B | texts for the visually impaired | edited |
| WR-P-E-L | tweets | unedited |
| WR-U-E-A | chats | unedited |
| WR-U-E-D | sms | unedited |

: An overview of the SoNaR subcorpora and our edited-unedited judgement {#tbl-sonar-editing}

[^no-attestations]: The newsletters subcorpus is incredibly small, hence why we have no attestations.

Adjectiveness information was added for all participles. Adjectiveness is expressed as a ratio denoting how often a participle functions as an adjective in language use:
$$
\frac{\text{\#uses as an adjective}}{\text{\#uses as an adjective + \#uses as a participle}}
$$

$0$ denotes no adjectival use, $1$ denotes maximal adjectival use. We computed adjectiveness on the entire Lassy corpus [@van_noord_large_2013].[^adjectiveness-dataset]

[^adjectiveness-dataset]: The adjectiveness dataset is published as a separate dataset for re-use by other researchers. See @sevenants_adjectiveness_2023.

Because the Alpino syntactic parser marks separable verbs by infixing an underscore (`_`) between the preposition and verb root, we can exploit this behaviour to automatically infer whether a verb cluster contains a separable verb.

We included frequency information from the SUBTLEX dataset [@keuleers_subtlex-nl_2010] in order be able to assess the effect of frequency. Because frequency is typically Zipfian [@zipf_psycho-biology_1965], we transformed the frequency information using the natural logarithm for a multitude of reasons: (i) to compress the frequency variation among the types in our dataset (ii) to make the distribution of priming more normal (iii) because it makes the distribution more psychologically real.

Priming information is also important to include. To obtain priming information, we relied on the sentence IDs included in the SoNaR corpus. Consider the following example:

::: {.center}
WR-P-P-B-0000000103.p.37.s.4
:::


The ID refers to document 103 of the `WR-P-P-B` component of the SoNaR corpus ('books'). Within that document, it refers to the 4^th^ sentence of the 37^th^ paragraph. The window we chose for priming is one paragraph: this means that in our example, we would consider all attestations from paragraph 36 and all sentences leading up to sentence 4 of paragraph 37 to be possible prime sources. It was not possible to work on the sentence level, since paragraphs can have a variable number of sentences and not all sentences have red-green attestations in the dataset.

We included priming in our model by using a modified odds-ratio [@bland_odds_2000] we will call the 'priming ratio'. For every attestation, we computed the following equation:
$$
\ln \left(\frac{\text{\#red primes} + 0.001}{\text{\#green primes} + 0.001}\right)
$$
We computed the ratio between the number of red and green primes and used Laplace smoothing [@brysbaert_dealing_2013] to prevent division by zero. The natural logarithm attenuates large disparities between red and green *and* turns our priming ratio into a continuous variable ranging from $-\infty$ to $+\infty$.

As a final step, we removed all participles for which no adjectiveness value was defined, as these were found *not* to be participles but mis-taggings. In addition, all attestations for which no region information was defined were also removed, because they lack the information required for the multifactorial analysis. As a result of these two steps, another `r subset(filtering_numbers, name == "second_removal")[1,][["no_items"]] %>% formatn` items were removed.

#### Converting the dataset

To compute the semantic preference of the participles found in our attestations, we used elastic net regression, the technique detailed in @sec-elastic-net-regression. In contrast to regular regression techniques, a tabular dataset cannot be used 'as-is' for analysis with elastic net. Instead, the dataset has to be supplied in a matrix form. Consider the following toy example:

| Word order | Participle | Country | Adjectiveness |
|---|---|---|---|
| green | gebroken | Belgium | 0.5 |
| red | mislukt | The Netherlands | 0.4 |
| green | gebeurd | Belgium | 0.1 |

In the matrix form, each multidimensional column is converted so that each unique value of that column becomes its own predictor. In our case, all unique values of the column `Participle` will become binary predictors, each predictor indicating whether that participle occurs in the cluster or not. This means our matrix will be inherently sparse, since each cluster can only feature one participle. Binary predictors such as `Country` are also converted to a binary column in the matrix, and simply indicate a deviation from the reference level. For example, if Belgium is the reference level, a Belgian attestation will be encoded as `1`, and a Netherlandic attestation as `0`. The `Adjectiveness` column is numeric and can be adopted as-is. The response variable `Word order` is also encoded as a binary variable, as is typical in logistic regression, but it is not a part of the input matrix. The input matrix for our toy example would look as follows:

| is_gebroken | is_mislukt | is_gebeurd | is_BE | adjectiveness |
|---|---|---|---|---|
| 1 | 0 | 0 | 1 | 0.5 |
| 0 | 1 | 0 | 0 | 0.4 |
| 0 | 0 | 1 | 1 | 0.1 |

The response variables would be encoded as [ 0, 1, 0 ] with the red order as the reference level.

To facilitate the conversion process, we used *ElasticToolsR* [@sevenants_elastictoolsr_2023], an R library written for this study. It can automatically convert 'traditional' datasets to the matrix format detailed above in seconds.

### Xpath queries

**Xpath queries for identifying eligible clauses**

Red order:

```{.r .code-overflow-wrap}
//node[@cat="cp" and node[@rel="cmp" and @pt="vg" and number(@begin) < ../node[@rel="body" and @cat="ssub"]/node[@rel="su" and @pt="vnw"]/number(@begin)] and node[@rel="body" and @cat="ssub" and node[@rel="su" and @pt="vnw" and number(@begin) < ../node[@rel="hd" and @pt="ww"]/number(@begin)] and node[@rel="hd" and @pt="ww" and number(@begin) < ../node[@rel="vc" and @cat="ppart"]/node[@rel="hd" and @pt="ww"]/number(@begin)] and node[@rel="vc" and @cat="ppart" and node[@rel="hd" and @pt="ww"]]]]
```

Green order:

```{.r .code-overflow-wrap}
//node[@cat="cp" and node[@rel="cmp" and @pt="vg" and number(@begin) < ../node[@rel="body" and @cat="ssub"]/node[@rel="su" and @pt="vnw"]/number(@begin)] and node[@rel="body" and @cat="ssub" and node[@rel="su" and @pt="vnw" and number(@begin) < ../node[@rel="vc" and @cat="ppart"]/node[@rel="hd" and @pt="ww"]/number(@begin)] and node[@rel="vc" and @cat="ppart" and node[@rel="hd" and @pt="ww" and number(@begin) < ../../node[@rel="hd" and @pt="ww"]/number(@begin)]] and node[@rel="hd" and @pt="ww"]]]
```

**Xpath queries for retrieving verb cluster participle**

```{.r .code-overflow-wrap}
//node[@wvorm="vd" and @begin $SIGN$ ../following-sibling::node[@wvorm="pv"]/@begin | ../preceding-sibling::node[@wvorm="pv"]/@begin]
```

with `$SIGN` = `>` for the red order, `<` for the green order

**Xpath queries for retrieving verb cluster auxiliary**

```{.r .code-overflow-wrap}
//node[@wvorm="pv" and @begin $SIGN$ ./preceding-sibling::node/node[@wvorm="vd"]/@begin | ./following-sibling::node/node[@wvorm="vd"]/@begin]
```

with `$SIGN` = `<` for the red order, `>` for the green order